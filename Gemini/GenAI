'## Transcript\n\n**0:00** Host: Hello everyone. It\'s our great pleasure to host Professor Stephan Zorn in our Bank of America Quant Speaker Series seminar, and we are very honored to have such a prominent speaker, and also, so many nice papers on machine learning and in many other fields. And Professor Stephan Zorn is an associate professor at Engineering Science at Oxford University and also, holds lots of other positions, both in Oxford University and in Oxford-Man Institute. And the talk will be on Generative AI for Limit Order Book Modeling. So we look forward to learning from you, and uh we really hope that uh we\'ll be like fascinating talk. Yeah, I will pass over to you.\n\n**0:55** Speaker A: This uh microphone podium. It\'s easier to just walk off from the podium, basically, it\'s probably safer. There, so yeah. Thanks a lot for everyone attending here in person as well as everyone attending online. I\'ve heard there is a great number attending online, so that\'s uh yeah, very um very honored to have such a big audience. So, today I want to speak about Generative AI for Limit Order Book Modeling. Uh in particular we will end with a very recent paper from end of last year on this topic, which is the paper with a number of collaborators in Oxford including Pierre Naggi as well as uh other researchers, but I will start slowly, and I will also cover uh a little bit the basics, kind of starting off with some kind of what are limit order books, and relevant definitions, which obviously for this audience might maybe be uh yeah, maybe a bit uh most of you might know a lot about limit order books, specifically people working in market making or execution, but there might be a few details particularly from the deep learning perspective which might be often overlooked, so I will just keep it brief but give a bit of an overview, and also cover some of our earlier works in the area on predictive modeling, before then building up on it, and moving on to uh those more recent generative AI techniques. So yeah, this is roughly the the structure. First, a little bit of an overview on microstructure in general and the older series of forecasting models based on limit order books, and then moving on to the more recent generative AI models. And maybe, just a few words on motivation, and, I mean, if there\'s really one thing you want to take away from from the talk is that maybe what\'s quite exciting is that we have seen all this breakthroughs from large language models. Everyone has heard about ChatGPT and other things, but it is not necessarily the focus on language, which might be the most important thing for people in finance, but it could also be that what we might be aiming at is using the same techniques we use, such as the transformer, which is this the T in ChatGPT, but use those techniques, but apply them to time series, to financial time series, or to other financial uh problems. And in this case here, I have presented work uh at at past seminars about using transformer based models, say for momentum strategies. And in this case, we look at generative models, but it\'s it\'s effectively the same techniques used in language models, but not applied to language, but applied to some form of market data. And in our case, it will be exchange messages, but I would say if there\'s one thing that don\'t necessarily just focus on on the language models applied to language, but think how those techniques can be useful for you uh in in other contexts, and I think that\'s where there\'s a sort of potential generally speaking. So this is just an image of the the computational graph um of some of these large language models, uh is provided by Graphcore, this image here. So um let\'s just move to microstructure data, and yeah, I will keep it brief for this audience there. I mean maybe just making clear what a limit order book is, I think most people uh in this audience know. So, we have orders resting at an exchange, and we have those levels here. People want to buy, they try to put in an order at a lower price here, and hoping that someone will um want to buy from them at this lower price, but if there\'s no matching order, the order will just rest and stay at the exchange. Similarly, people trying to sell, they place orders at a higher price because they want to sell it at a higher price. If there\'s no immediate match, those orders will just stay at the exchange until someone comes in with a little bit of less patience and crosses the spread. It will eventually take that liquidity. So, those are the passive orders in the limit order book, this is the resting liquidity at the limit order book, and you can think of the limit order book is just a kind of state the exchange keeps track of to be able to do its matching. So, what the exchange obviously receives, it\'s a different type of message stream, people placing orders, and canceling orders, but internally, the exchange keeps track of those orders in this limit order book, and that that representation is quite useful. As we know, for example, there could be some potential pressure of more people wanting to buy building up lower in the order book before a price is actually changed, and if you could somehow pick up those patterns, we might be able to have some edge in forecasting prices for example. So yeah, this is the limit order book, and the mid price, which we see on the screen, is just this intersection between this top level, which is the best bid and the best ask uh there. And then obviously uh when we see this price changing, something is happening. For example, in this case, we could have a buy limit order coming in at a price high enough that it matches some liquidity which was already there. And in this case, we see that this order uh disappears, and what we have is if it was a limit order, we see that the remainder of this limit order actually staying on in the book, and as a result, the mid price is changing, and what we also see is that the price changes are certainly the consequence, and we have two trades recorded here, just because there are two different sell orders, so each are different counterparties, so we have one trade recorded for this one, and one trade recorded for this one. What the exchange has to do is just do this matching and record the trades, so everyone knows uh where the money is exchanging, and where the financial instruments it\'s going. But, obviously, it helps for us to understand what is going on uh behind the scenes. And obviously, as you know, there\'s a whole other bunch of orders, in this case, the limit order kept the remainder, it could have been an IOC, for example, in which case there would have been nothing remaining, if we would have had a wider spread. But generally speaking, it is quite instructive to look at those limit order books when trying to make predictions of where prices are moving. And, this is exactly an area where people have been looking into applying machine learning techniques. And just for the terminology, um So, there are different types of data. People often refer to the L1 data, which is the top of book data, and often times when people speak about L1 data, they also include the trades there, which is some additional quite valuable information as well because obviously when a trade happens, money really exchanges hands, which is not necessarily the case for some orders being placed and canceled on the exchange. So, there\'s some more commitment in there, so potentially there\'s some more alpha to be extracted potentially from trades, and this is often included in the data stream of L1. If you ask a data provider for L1 data, you probably get the top of book plus the trades um attached to it. And again, not every top of book change will there be a change a trade happening. It is only a fraction of uh of updates that the trade actually happens. The next one here is the L2 data, which is commonly referred to or as limit order books. And this is what usually if you look at academic literature in limit order books, it usually refers to those uh L2 data or LOB data, and I only wanted to make the difference because actually there is this other view here, the the L3 data or the full limit order book data, which is something which hasn\'t really that much been explored in the academic context, which is the exact content of the order book. This is also something important to some extent, because it has additional information, but it\'s something actually harder to model. You can imagine like well how hard it is actually to model this one here with a data driven approach, approach, what kind of representation do I use to model this type of limit order book data which is something we will get into later. But it might even be more complicated to model this type of data, because it\'s not a really clear structure, because in principle, the number of orders on each level is not fixed. But, you can very easily imagine that there is additional information in this data, even if it looks the same, this and this, whether this is one big order and this is a hundred small orders. There is clearly some difference there in information content, and also where the price might be going. It might also be interesting to know this order at the top, when was it actually placed, was it placed very shortly after a price changed, uh or is it something where um was it placed maybe deeper in the book without much relation to price changes, so this is something quite interesting, and we actually have some ongoing work which is hopefully coming out in a month or so, where we use some um unsupervised learning. So obviously, we do not know the counterparties here, but what we try to do is we do some clustering using some features such as time since the order was placed, since the last price move. What we try to cluster those orders into different groups, could it be a market maker and HFT, could it be some institutional client, and like seeing once we have clusters, do these different clusters actually have different predictability, is it maybe that a cluster of HFT\'s has more far um imbalanced, this cluster has maybe more forecast ability over a shorter period of time than they say the institutional flows. So, there\'s some interesting structure here which you can extract, and we have some ongoing work on it. I just wanted to flag it here because if you go into the normal kind of traditional academic literature, limit order books, it usually just stops here and people don\'t even question what is beyond this. So, I thought it\'s actually good to just point it out uh in this introduction section here. But obviously this data structure is quite complicated. If you think about how to store it in a data frame, because it it is not really clearly defined how many orders there actually are. So, the way how this data is usually uh uh broadcasted is in this MBO stream, which is the messages there. It\'s the individual orders coming in which incrementally change this limit order book, which is how you usually receive this data from an exchange. So yeah, uh for the first instance, let\'s just focus then on the limit order book data, which is as I said, what most academic work is focused on. So, just quickly to recap, this is a picture from our DeepLOB paper, it\'s already a bit older paper, now from 2019, uh but it it it has been quite popular in in um applications by practitioners, and also some follow-up papers there, some interesting work by Petra Korem which looks at order flow imbalances. But effectively, they use a similar architecture to what we use here in DeepLOB, while but don\'t apply it to the full limit order book data. So, what we tried to do is, it\'s say okay, what if we wanted to do some forecasting, and we want to read in limit order books, or say not just one limit order book but say a sequence like the last 100 limit order books. How do we actually do that? How do we process this data? So the way how we did it here, we we looked at say hundreds limit order books, and we use tick time here. Tick time is nice event time, because it\'s somewhat self-normalizing. If markets are a bit more active, ticks happen faster. If markets slow down, ticks are more sporadic. Or across instruments, one instrument is just a bit more liquid than another one, but it just happens that time somewhat runs faster with this instrument. But, if you use tick time, it\'s somewhat self-normalizing, and it helps when building one model for uh a a pulled class of instruments at once there. So, we take this limit order book, and we look, each column here is one limit order book. And um what we have here, in the rows, we have here the prices 10 on each side, so there are 20 rows, and sizes 10 at each side there. For most equity markets, if the order books are dense, you wouldn\'t really have to take track of all the 10 price levels. You could just say, I keep track of the mid price and the spread, that would generally be enough. But, here, we kept it generally and we just kept track of all the prices. And, those are the 100 ticks. And what do these colors represent? So what we do is So, again, normalization is key. If you want to build a deep learning model, a lot of the exercise we need to how can we actually normalize the data, and be able to build a model, especially if you want to have one model for different instruments. And what we do here, we keep some form of running average, so some running Z-score, and we normalize everything by some running Z-score. So, if it\'s a brighter color, it means it\'s higher than its recent past, could be over the last 6 hours. We keep the value, and then we see is it higher or lower, uh and darker color means lower. And, you can actually see some interesting pattern here. This is the sizes, so here we see increased size on the bid and slightly later, the price actually went up here. And you see those colors mainly move in sync which is what I mentioned, we wouldn\'t nearly really need to have keep track of all the 10 level prices, probably mid price and spread would be enough in practice. But this is what we do here in this paper, and we say, well if we can somewhat spot by eye what\'s going on, surely some algorithm ought to be able to do that, and given this representation, which is just a data frame, but it looks a little bit like an image, we think that actually techniques from image recognition can be quite good. And we actually use some convolutional filters here. The basic idea is if you\'re good to learn an imbalance here, the same definition of imbalance ought to work here as well sometime earlier. So, we wouldn\'t have to relearn imbalance, and that is exactly what kind of convolutional filters do. You you you learn some joint filters, which in images are kind of like if I learn a feature for this part of the image, the feature should be the same for the other part of the image, that\'s the basic idea here. So this is the basic set up, and I don\'t go into all the details because it\'s some earlier paper, but basically we have some sequence of limit order books, and we want to make some predictions of future price moves. This could be returns, uh in a regression problem, it could be quantiles uh of returns, it could also be a classification problem where we threshold it and say if the price goes above or below a certain threshold. And obviously, we have to set some horizons. Sometimes, we look at 50 ticks into the future, 100 ticks into the future, or you could also say, well maybe I want to have a whole range, I want 50, 100, 150 to 200, because maybe for the problem I\'m I\'m interested, there could be some maybe some price drift, and some reversal, so maybe I want to know how the price is evolving. So, in general, it could be some multi multi horizon prediction tasks there. And this is exactly what we did in our earlier work, and I just wanted to mention that there, we already found some analogy with language models because effectively, the structure we have here is quite similar to what you have in some translation algorithms. We have some English sentence, there\'s a model which builds up a state of this sentence here over time, and it creates this thing which is known as a context vector basically, which then it, and then using the context vector, we now start to reconstruct this sentence in a different language, in this case French, for example. And this is exactly how language models work, the way how you train it is you you fix this one here and you fix the input and output, and the model has to optimize its parameters, so it can learn a useful representation from which it can reproduce the sentence there. So, the this context vector is a bit like some artificial language the computer learns, or the machine, the algorithm learns, from which it can then reproduce the sentence. It\'s a bit like it translated to its own language, and then from its own language to French there. And from a finance point of view, we can think of it: this is the state of the market. I want to somewhat take the input data, learn the state of the market, and using the state of the market, I want to make predictions, basically. This is the basic uh set up. And technically speaking, in language models, this one is called the encoder, the other one is called the decoder. And, those models here, um this is a sequence to sequence model, very typical model for translation. And our, some of our earlier works, we actually showed that this is a model which you can use for this limit order book forecasting. And what we have also seen is that you can do a bit better if you use attention. One problem here is this is that because it passes information one to the other, it\'s very difficult for the model to learn long-term dependencies. If it was relevant here, and then irrelevant here, the model will kind of forgets the information halfway through, so it\'s not really good in learning long-term dependencies. But you can rectify it if you include what is known as attention. And the meaning is very simple, it\'s exactly what it means, you have those extra weights, which is not just looking back at its last memory unit, but you can look back a given number of memory units, and you can learn to pay more attention to something further in the past, and that helps you to deal with certain long-term dependencies. So, this is exactly how uh attention works. And now we have a a set up of sequence to sequence learning and attention learning, and this is something we did in an earlier work, and I won\'t show all the results which are in the paper, and I had presented it at other events because this is not really the main topic of the paper, but I just wanted to say that we did have quite some works, the DeepLOB paper, which is the one where we initially introduced this architecture with uh which uses RNN\'s and LSTM\'s as well as this um follow-up paper which was published in Risk as you might be sitting here, uh which um is a one which looked at this multi horizon forecasting where we already make this link to translation algorithms, where we use those techniques from sequence to sequence and attention which are used in translation to do better forecasting. So I just wanted to quickly give like some background over those models, and and and show that in those older papers, we already made links to language models. But now, what I really want to do is move to this new work on generative AI modeling, where we really take it take it one step further in the analogy to language modeling, which I think will be quite a bit more exciting. And, as I mentioned, this work is based on a recent work from um from um last September. It is a work which was published in the ICLR conference, and actually it\'s part of two papers. You will see later that to be able to build the generative AI model, we actually had to build a a microstructure simulator because it needs simulation, it needs to, you need to rebuild the limit order book by replaying the messages and be able to build up the limit order book. And because we need to apply it many times in the training loop, we needed to speed it up, and actually build a a a limit order book simulator which runs on GPUs. So it it this is actually part of two papers. The one paper which is this one here, which is the generative AI model and the underlying work which is actually the GPU based market simulator, which is the one which won the ICLR best paper award last uh year, so I I didn\'t expect anyone be so interested in in limit order book simulators to give it the best paper award, but apparently people like those limit order book simulations. So, yeah, I\'m not alone with uh liking limit order books there. But yeah, let\'s get to it. So, we already got to know the L3 data, so that\'s good, everyone now knows what L3 data is, but as I mentioned, this L3 data doesn\'t come like this. We can\'t really easily kind of give it to the algorithm, it\'s not a very nice data representation, and it also doesn\'t come in this is very big if you were to actually have it in this form um every update. So, the way how it actually comes is in form of this uh MBO messages there, so And this is how we look like. So someone comes and places order to sell 10 lots at 10.7. So, you see the exchange just places this order, and now it\'s in a different state. And now someone else says I want to buy 30 lots at this price here, and a new order comes for me, this is the top of the book, this is the end of the book further outwards. And, you see what you really see are those messages here, and another one up there. Now, those messages they are just the incremental changes, and they are a bit easier to process, but they they are some funny ones there which are not so intuitive. And, the deep learning algorithm doesn\'t really like them, or any algorithm for that matter, which are those ones, for example. Cancel order 474920 there, that\'s not very useful information, obviously the exchange knows what to do because it actually has a hashmap of all the orders, and it knows which price level to find it. It\'s at 10.9, and the exchange goes to finds it and takes it off the level basically, but for the algorithm, that\'s not very useful, uh that\'s the exchange can do it. So, it\'s not very nice to to model this, we don\'t really know much about it, so we have to think about some ways how we actually represent this data a little bit better. And uh yeah, now the order\'s gone basically. So, what we do, we do some form of encoding of this first, like some intuitive encoding. So, it makes much more sense for the algorithm to say, okay first, I come up with some codes like place, it\'s action type one, sell is action minus one, buy would be action plus one, and then say 10, well this is 10, basically. But, then, one thing is, for example, these absolute price values are not very helpful, basically. So, uh place an order where 10.7, well 10.7 is one thing now, 10.7 is a whole different thing tomorrow. Uh What you really want to know, how far away is it from the mid price there, so it\'s actually one tick up, it\'s on level one basically, level plus one, this would be level plus two and so on. So, it is much better to give the prices where the orders are placed relative to the mid price in units of ticks because that\'s much more intuitive. Isn\'t it at the top of the book or deeper in the book, that has a very different meaning, and that\'s much more useful for the algorithm to know. Then obviously, there\'s some ID, and some time actually, or this message just appears at a given time. I ignore what\'s the time uh so far. And, I can do the same here. Now, it just buy is action plus one, 30 level minus three here. And then, um of some ID. So, this is just some useful encoding, and when I have to cancel order, I can just augment it with this information. We know that canceling it just needs to give the ID. Fair enough, that the exchange can find it just given this ID, but our algorithm would like to see some more information, so we just give it to it to say okay what what the person wants to cancel is an order which we have identified, is at level plus uh it it\'s a cancel action, it\'s type three, it\'s cancel, the cancel is actually a sell order of type minus one, it was actually of size 50, and it\'s now actually sitting on the third level there. So, we can just give this information, we can look it up and find it, and now it\'s much more useful for this information, for example, the algorithm can see, here someone is adding liquidity on the on the on the sell side, adding liquidity on the buy side. Here someone is taking away liquidity of a given unit from the sell side, deeper in the book. So this is much more useful representation for the algorithm. And, now we can hope that we just kind of stream this messages and try to use this information. In effect, we did this already some while back, where we looked at some forecasting models such as DeepLOB, and see can we actually just, rather than using limit order book states, use the stream of messages in some normalized form like this, and take this as input and and we actually did this, and we found there was some additional information content the model could make use of, which was orthogonal to just the limit order books. So, if we added this model to the limit order books, we increased our forecasting performance there. So, the limit order books are good representations, but there are some additional e representations in here, which have some useful information content which the model can pick up. Something like the thing I mentioned earlier. If you have size 100 and size 100, it seems to be no imbalance. But if these guys, a hundred guys wanting to buy, and this is just one, maybe the fact that 100 people want to buy, says us something, basically. So there is more information in the full books, and this is something we can see it through we we can build models in this type, but it\'s interesting to see that they actually get better in forecasting. But, what we want to do now is actually not look at this forecasting stuff, but we want to go a bit further and build this generative AI model. And, the way how you should think a bit about it is if you have those sentences here, you see a sentence like: "Never in the field of human conflict was so much owed by so many". And you ask, okay, how does this sentence continue? This one many of you might have guessed because it\'s a very famous quote from Winston Churchill. So, I guess many would have figured it out, and even some ChatGPT would have probably been able to tell you how this sentence continued. But, the question is, can you actually do something similar with this market order? What if I have those messages, rather than having words, I have messages like: place sell at 10 lots, place buy of 30 lots at this price. What is the next one? And you can imagine that this is somewhat possible. Obviously, languages the reason why you can continue this sentence is because there\'s obviously some very intricate relationships between all these words, which helps you to figure out some meaning of the sentence, and then be able to continue it in a sensible way. But we also know that there exist relationships of similar type also for messages. I I pointed out in some discussions earlier, there\'s some very interesting work also by Bushou on microscopic impact, where you can look at actual correlation functions. How much if I put a large if I place a large limit order to buy now, how does it actually potentially increase the probability that someone wants to cancel one tick, 50 ticks, 100 ticks into the future. And, there are actually very subtle correlations, and those correlations in aggregate lead to price impact because if I do something, someone is likely to maybe, if I place and try pushing a bit the market, maybe someone will cancel on the other side of the book, and as a result, the price moves the way it\'s very subtle correlation in between all these different events. But, we will could hope that if you present enough data to a model that it can pick up those relationships, and that\'s exactly what we want to try in our work, can we come up with a model which can actually do that. And again, there are some further normalization going on. So, this is a bit of a complicated slide, but let me try to walk you through it. So, what we do, we take some other tricks from language models. Language models use something which is called a tokenizer. It tries to con convert words or parts of words into tokens, which is some kind of countable object basically, and it then uses those objects, now it has a very nice dictionary of say numbers, and you can work with those objects. We want to do the same because this allows us to assign some cross entropy loss because now we can uh make it a little bit like we have different classes, which type of message is it, we have something at least which is like nice and finite, and countable, and we can assign a probability to each of these possible outcomes there. And what we do, we have to tokenize it. So, uh as I mentioned for the first, the first is a type here. So, we had like place, cancel uh the different types of um or modify. And we just assign an ID to that, so this this one here cancel just the tokenizer assigned some number to it, it\'s 1005. Doesn\'t really matter what it is, it just means that it\'s a unique number, basically. And the same is true for this one here, this is the direction we had, buy and sell, and again the tokenizer assigns a unique token to each of them. The important thing is that action one and type one do not get assigned the same numbers there because they're different types. They get a different dictionary because a buy is not the same as as a placement they even though I use the same number one but they're using different contexts. So I can't overlap my vocabulary here in this case. Then, I have prices. Again, I used the prices relative to the mid price. We go up and down 100 ticks from the mid price and just assign it a token for level minus one or whatever level it is up to plus minus 100. And then, we have the sizes as well which get tokenized again. The vocabulary is non-overlapping with the other ones so the size one is a unique token which is not the same as the token used for buy for example. And then, we have some other quantities. These are the ones we ignored a little bit which are times, but if we're actually simulating market messages we want to know whether the next message came in like half a second later or uh a millisecond later. And actually, we we we have to encode this well the time of the messages. And those are very long numbers in in nanoseconds and they get tokenized into various tokens here. You see there's a bunch of tokens to tokenize this longer number in nanoseconds. And now, what I mentioned earlier we had this issue where some cancellation actually referred to an older message. I just had the idea and obviously, it might be useful to know some properties about the order I'm actually canceling. This is why whenever we have a cancellation, we add the reference matrix uh reference information here. In this case, we are canceling an order and it's now nine levels down but when it was actually placed, it was eight levels down and it was placed at the same size 100. It's a full cancel but it could have been partially canceled down already before and it was uh at this time difference basically uh at this time before. So we actually have this time information as well. So each order each message has its own information which is enough if it's a placement. But, if it's a cancellation, it will also have the information from where it was originally placed which is quite useful information to have as well for the model. And then, what you get is this really large string of tokens there and this is how the message is come in. So, effectively we have this tokenizer we tokenize all these messages and now those tokens are streamed to the model, basically. So, um so this is how it is built. And, here is kind of where the architecture comes in. We have this stream of messages here and we go um uh 500 messages into the past. So we have the last 500 messages coming in. They are tokenized they go into the model and then we use something which is a S5 is a state space model. It's something quite interesting uh and I won't go much into the details, but it's something which has been uh uh started to be used more and more in times series modeling rather than having a transformer-based architecture with attention. It is a state space models which is very good. I mean if one thing you want to take away is a very efficient model which is good at learning long-term dependencies. And we saw this state-based model is a quite good fun for this type of data. And then, what we do is actually we also found from some of our previous work and from experiments in this one that limit order books are very good representation of the data. We can already see some imbalances by eye building up. So we can definitely helps the model by giving it the limit order book snapshot. The information is somewhat here. If If you have a long enough stream of messages you could rebuild yourself to limit order books but it's just additional an additional requirement that we have to force the model to learn this representation itself if we already know that it is a good representation. So we also feed in this limit order book which we process with a separate model and then these models here are combined and what the model is trying to do is try to predict the next message basically. And the way how it it does it because we have a tokenization we can use uh uh a cross-entropy loss like is if it was a classification problem there. Just is like a a finite number of classes it has as options here and we'll give gives us a probability for each of one. So, the models' output is a probability over messages and we just sample from this one. Now, we can sample a new message which could be this message here and this would be the next step prediction. But, what we can do nothing is stopping us now from sticking this message back into the model updating the limit order book and running it forward one more step and one more step and one more step and continuing running it forward. That's exactly how the generative model works. There are a few caveats here and one caveat is that at each step we need to simulate the thing. Uh we need to replay the limit order book as if the exchange would do, so we need a good simulator which is something I mentioned earlier which we built in some earlier work. And there is this form of you might call it hallucinations to kind of connect to some phrases using like language model. But, there is just things which can go wrong here where the model says well, I want to cancel the order of this idea and it's just not there basically. So so uh and and in this case we can basically say well, this is not a valid answer. Let's go again, basically. So, we have this error correction loop where it tries to correct itself and we have been seeing that the better the model gets the more data we have the need for this error correction gets less so it gets much more sensible in the type of data it produces. That in a nutshell, I know I'm be I'm skipping a lot of details here, but it kind of just to give you the intuitive idea, this is basically how the models work. And now, we have like some six months data here for training and we used Google and Intel, just to like, has the two Nasdaq stocks one is actually a a um large tick and a small tick uh stock or the other way around actually. So we have like two different types of instruments to see how the results look different for large tick and small tick stocks and also Google is a very liquid name so we also see I mean we wouldn't expect to be as much predictability as in other instruments. So if it works for Google well it hopefully works well for other ones. So we just tested it on those and as I said, what we always do is we take 500 messages as input then we predicts the next message by the model generates the probabilities we sample a new message and then we put it back in and do another time and we do this rolling forward 100 steps, so we can really take a snapshot of data and then run it forward for 100 ticks. Just using its kind of artificial um rule forward mode. And first thing to look at is kind of well do we get some reasonable distributions for these outputs there? For example, just something very basic like if you look at what is the fraction of new placements cancellations or deletions or the amount of actually trades being executed ideally we get somewhat similar \nproportions here than we see in reality and you can see actually the generated data and the real data you know obviously we have historical data. We just stop here. Now, we run it forward using the generative model for 100 ticks, but we also saw what happens in reality in 100 ticks and we can do the comparison here generated in red and the actual one in blue and we can see that the statistics here align reasonably well. We can do similar things as well if you look at the the range of price moves here. This is just the evolution of the future mid price, could be that maybe our model just the price never changes or the volatility is three times as high and it's obviously non-trivial if you if the model only does one step at a time and you run it 100 steps into the future. If the model is a little bit off you can easily imagine how the whole thing just blows off basically and suddenly it's all over the place. So, it's quite interesting to see that we actually get quite overlap a good overlap between the actual mid-price evolution and the one of the generated price move here. And, even for the times the times actually I mean if if you think about in the tick space you don't really care about the times you just say there's a new message let me just process it. I don't mind how much time actually passed, but our model also generates the time there and this is a quite difficult task, we see we needed quite many tokens to represent the time and it actually at the time is the only ones which has shared tokens across times and you see that the distribution of times between messages is actually quite closely matched. So, this is also a non-trivial one because there was a lot of stuff which could go wrong with those uh time differences, so we were quite happy uh when we saw that and quite This one here. Um Ye yeah, this is a very small time. I think it's just maybe subsequent orders as there is probably I think it's just a time limit when you have a um I think it's just the execution of multiple like there's one order coming in and it executes against multiple other ones. For example, like you saw in the very beginning you have this one limit order coming in or one IOC and it trades against two different orders so they're kind of recorded at the same time which is a in our granularity is just shown here basically. This is in a like instantaneously which is just one action which might cause several trades subsequently there. So, this is why it's shown it's it's like smallest time interval possible basically. Perfect, and And, uh yeah, what was quite interesting is that this is obviously nice if you think about we generate artificial data so it's actually quite nice that we get some reasonable distributions, but one thing we can obviously ask well, if there's some form of imbalance in the data and if the model really tries to pick up those things like impact of some previous order which is not trivial that the model would pick it up we might actually be able to see some form of correlation with future price moves and we can actually see that this model shows some quite good if you compare the mid-price evolution, this is like 100 ticks into the future and we look at the correlation of the mid price it shows actually quite some significant correlation here for for for Google. It becomes insignificant after like 20 ticks, but then, for Intel it persists a bit longer be again it's because it's a large tick stock, but it's quite interesting to see that there are actual correlations. So, if you can imagine if there's some maybe a lot of buys happening which we expect to eventually push the price upwards that the model picks up those inc uh increases and then is able to in its extrapolate the sequence of messages actually maybe favor cancellations on the other side of the book, which cause the price actually to move upwards there. And, those are the only results we have on this type of correlations in the actual paper, but we are currently looking in some follow-up work because obviously as I mentioned um this the main reason for building this model was for simulation studies of counterfactuals. Uh It's It's not just for the sake of generating more data, it's for having data in in situations which we didn't see in reality. I can look at historical data, but if I'm simulating execution algorithm and I place a large market order I don't know what would have happened in reality if I had placed this large market order. There's obviously the indirect impact, which is not captured if I was were just to replay my historical market data uh which didn't see this large placements there. So, to be able to have a good replay, we want to be we want to use this generative uh AI model. That's exactly the purpose why we built it. So, one thing we're looking at right now is does it actually reproduce price impact. What if I now aggregated it and what if I pretend I executed a whole market order over a longer period of time, a meta-order over a long period of time. Would I actually get the right aggregate impact of this meta-order? This is something we're looking at right now which is non-trivial because we don't put this in by hand. One thing is if you say I built a generative model and I say I have a a,  gone model and I fitted to match the first four moments, and now it matches the first four four moments, yeah well you built the model so it does it, so there's no big surprise here. But those are all things we haven't built in, we haven't put in that it should have correlations with the future mid price neither have we built in any of this impact relationships there. And, you know impact is a bit like if you are a physicist, it's a bit like statistical mechanics and thermodynamics so you know it's it's non-trivial that the laws of \nmechanics in aggregate generate a phenomenological law of a gas, basically. It It's a non-trivial relationship and and if we can see those relationships appearing, we get much more confidence in those models there. So that's something we are doing right now in follow-up work. We are also building a model where rather than testing right now, our loss function is only next message so or next token to be more precise. So, each time we check is a is an autoregressive model where the loss is only looking at the next token but we see it's actually quite good in generating whole sequences of tokens or messages, 100 messages into the future. And, what we are right now doing is also building a model where it actually has an additional it has an extra critical model which actually tries to then interrogate whether the whole sequence is actually good. Not just the next message is, but I have a loss function for the next message, but now I can step back now you generated 1,000 messages. Is the whole sequence of 1,000 messages actually a good one and we can have some extra step of learning to ensure that. These are all some works which we are looking in right now and maybe in a future uh seminar I might be able to update you on those. Now, towards the end, I just wanted to quickly comment on uh where those models are actually used for. Sometimes people wonder well, I mean it seems to be quite some good forecast ability um like why don't you just switch your money on and become a billionaire something like this. So so is it too good to be true? One thing to mention is obviously if you are forecasting things say 50 ticks into the future, 100 ticks into the future, there's only so much the price can move in this period of time and obviously if you were to say aggressively uh now, aggressive is not the word you may like so much in this presentation but take liquidity uh uh then you have to obviously overcome the spread and maybe crossing the spread here and crossing the spread there is actually something which can be very costly where maybe most of your signal is is gone. Now, so this type of signal is quite good when you look at something where you say in an execution algorithm or a market-making algorithm, but in the execution algorithm but you have to trade anyhow, so we have to trade this large quantity, we have to pay the fees anyhow. So, we don't have to overcome uh those, but there it's a whole different story where actually by maybe just anticipating or delaying certain orders we might be able to largely monetize such an order, but obviously only on flow which we are already trading anyhow. And, similarly, if you are a market maker you are also generally capacity limited where you have to get some passive fills and only on those passive fills you got can you actually then monetize such type of signals. This is just to provide some context of how it is used and here is actually some um snapshot of an actual execution algorithm from from uh man group here, where you see this is the best best the best ask and you see these are passive orders which are placed and canceled here. So it's kind of chasing the market here. It got a fill. Here, it got another fill, but you can obviously see uh this algorithm here is a very passive algorithm. It just tries to uh save enough spread by being as passive as it can be and it doesn't use any uh predictive signal in here but you can see in cases like this, if you had some good predictability of that the market is going up at this stage you might very well try to cross the spread at this stage, a bit earlier rather than just chasing the market like it is done here. So, this is a type of application domain if you ask where are those type of models used, they would be used in execution algorithm at at um various places uh banks or obviously hedge funds which have bespoke execution algorithms or in the context of market-making algorithms. It is just such a the signal the the variability of the price over such short periods of time is just not large enough to turn it into a fully liquidity-taking strategy, it just has to be used to modulate existing either execution flow or passive flow coming in through a market-making algorithm. So, that's a kind of typical use case of uh such an algorithm. And, um finally, last slide, something unrelated, I just throw in we we actually have some other papers, some new papers which came out very recently and one is in progress which I just thought I mentioned related to LLM's, actually to do with language there, so I just thought let me mention them quickly. Uh the first one it has a funny name and I made this great image here with uh GPT, which is the one called Time Machine GPT and it's a it's a nice one which actually is a um something quite relevant, I would say, for people in finance which is often overlooked, it's a it's a point-in-time language model. The main difference is that um if you were to look at a language model and you said you wanted to use a language model in some setting of a quantitative strategy you wouldn't be able to backtest it, because you couldn't ensure that you don't have a look ahead bias, basically. If I were to build a model which uses uh news to forecast volatility it will do a great job when it reads about lockdown in 2019 in China, but in reality a language model 2019 would have had the faintest clue what lockdown actually is and was so well, let's fine, we're just going to continue and that that let's not bother, basically. Whereas, obviously language model now would be all uh all like oh my god, the world is coming to an end, basically. Similarly, for other things like and one or others. So, language models use all kinds of data, we don't even know what data runs into them and obviously we can't use them in conjunction with any other times series model and then be able to backtest. It just doesn't work and how are we going to trust them I mean just switching on the strategy without backtest and letting it run probably not there. So, what we tried to do is we have this point-in-time models, the sequence of GPT-2 models which has from 2007 til now, which ensures there's only data for this year up to this year is going into the model, basically. It's It's trained brute force by just um building a new model for every year, but it was quite some work which went into it to basically rebuild Wikipedia for every year by looking at all the page updates. So quite some work went into it, but I think if you are usually actually using this kind of language models in the context of finance you really want to be able to have this possibility to backtest them and I think for this such models are quite useful. And, together with some researchers from Princeton, we are also putting out a survey paper which hopefully comes by the end of this month, maybe it's first week of June, on kind of uh progress and prospects and challenges for large language models in finance with lots of applications including time series modeling and use of this market data like I've shown today. But, also the more traditional usages of language models using language actually. So, this is just some recent work and with that I um close and just summarize quick uh quickly. So I hope we have seen that some of these techniques which are used in language models such as attention and tokenization can be quite useful for finance, but not necessarily by processing language uh of financial news or something like this, but actually applying those techniques to market data, time series data, or even order books or exchange messages there, which in a sense are even closer to language than time series data because those are actual messages. They They have maybe more resemblance of language than other parts of of finance and I've showcased with some of our earlier works such as the developed model as well as some multi-horizon extension of that before showing that actually beyond modeling limit order books, it is really interesting to look at the actual messages which are coming from the exchange, which is called the MBO data. And we have then seen this generative AI model which actually resents those messages and tries to extrapolate the stream of messages going into the future which helps us to generate artificial data which is of similar type as historical data, but it's also conditioned to the recent past we have seen and this conditioning actually allows the model to have a bit of forecastability. I'm not sure whether I would use this type of models for in forecasting tasks but definitely it is something which can be of great importance if you wanted to simulate execution algorithms and have a good understanding of market impact when you actually place different orders in your simulations than you had in reality and you want to see how it's actually affected the market and we have lots of studies where we look into this right now. We see how can it reproduce market impact, can it maybe reproduce the flash crash? What if we put a really very big order, when do we get out of distribution? All all those are things which we are currently uh looking into and I think there's lots of interesting follow-up work to come out of that. And, with that I I thank you all and hopefully, we have some 7 minutes left for questions. Yes, so thank you very much. Well, thank you so much for such a great talk and uh before we start the Q and A session, I just wanted to remind everyone that given that we have participants from different organizations please do not share any sensitive for confidential information. So if you have any questions please raise your hand. Thanks. Um yeah, firstly thank you for that, very interesting. Um a question on the tokenization you used. Did you try different tokenizers and different encoding schemes and how did that like performance change? We kept it very simple then, we didn't try much I mean because we tried to uh kind of innovate more on the model side and not so much on the tokenizer there. But it's It's maybe worthwhile to get back and try different tokenizers but we were quite happy with the performance we saw out of the box there. Actually the whole correlation of future mid prices we didn't expect that, so we were kind of happier with that. So yeah, we didn't go back but it's just like once we go and get into more details and follow-up work, it's definitely something we can look into. Cool, thank you. Um Uh obviously this is a very high-frequency data. Um Have you ever tried it with lower frequency data? I'm I'm guessing there's probably not not enough data to train the model, but uh be interested to hear your thoughts on on lower frequency data. Well, yes I think the big question is obviously how do you represent the data here? Because obviously here the way how it's constructed as a language is obviously important that it's sequence there. It's like if I have a sentence and I said I give you only every 100th words, you wouldn't understand the thing, would you? And, so um it's it's the same thing. So if I if I downsample it then that's a different actually interesting question and just maybe just throwing some other things we want uh once had some students look into what's actually a nice way of downsampling data, because obviously say the last price shot is not a good summarization of what happened in the whole minute. What about the last imbalance, probably needs that maybe some aggregate but how do you actually integrate all this information, it's a good question and something you have to address this first before you do that actually. And then, 1 minute in 1 stock might mean something, but 1 minute in Google doesn't mean the same as 1 minute in Intel there. So by sticking with the whole stream of messages down to the tick level, no matter how much time that actually takes is nice because you have in a sense the full sentence there. Once you start downsampling it gets a bit more tricky and you have to ask all those questions actually, how how am I going to downsample it and uh how does it still makes sense? How do I represent all the information in between? It could be that and I mean one way of doing it uh that's what I thought for this project for a student which hasn't shown up yet is the it could be some form of encoder which processes the ticks in 1 minute and then maybe gives a summarization which is some internal state of this encoder or maybe something like an auto encoder which summarizes all the micro structure data over 1 minute to give me a good minutely representation there. But, you have to figure this bit out first before you can then make sense and then maybe uh a bespoke model could be built for this downsampled data. But, I do think this this type of model it makes more sense on the fine-grained level, exactly as the analogy it doesn't make much sense to look at uh at at language every hundredth words. So it's kind of the same thing, basically. So unless you have a good summarization of the text, then you can apply to the summarization. But it's the same thing, you have to solve the summarization first before you can address this problem, but it's a very interesting idea to pursue. You would do the summarization and then do this one there. Thanks. Uh just 1 minute yeah, just want to remind people on WebEx yeah if you do have any questions yeah please click the button, raise hand yeah I can unmute you. So, any other questions? So, do you have numerical results on how better reconciliation is between backtest and realized trading strategy performance? Well, how do you mean the realization between backtest just to make sure I understand it correctly. One of the things is that you could backtest better passive strategies. And, uh I was asking if you you have like numerical results on how much it betters reconciliation. So, as I mentioned we are currently looking into the price impact modelling and that is something which obviously helps you to reconcile between a simulated execution strategy and the which and the live trading one because the live trading one has the one the indirect impact in. Just making sure there's a direct impact which is I trade and I remove a level. This type of impact is correctly captured by the simulator. There's a form of adverse selection happening if you trade passively. It can be that I place an order, but I'm so bad in placing order that I only get a fill if everyone else canceled. I'm the last man standing basically. Then, I'm badly adverse selected and this adverse selection is also correctly captured by a a a standard uh limit order book simulator. So the standard simulator does correctly capture direct impact and adverse selection and spread savings. All these costs are directly uh are are included. The only one which is which is can't capture is the indirect impact and this is really where this model uh in and am I doing something funny, some flickering and people react to it. This is where you hope that the generative AI model can give you this extra edge, but this is exactly what I said we are testing right now. How can we actually see how much better can we reproduce impact uh relationships there? Because obviously if I were to have a simulated trajectory, no matter what I include, I wouldn't see any impact of this in aggregate. But obviously here, I hope that if I consistently keep on buying over a course of many ticks over a course of an hour that there will be some drift which is roughly like proportional to the square root of that quantity and and it's whether we can see this empirical price impact laws which gives us exactly this confidence that yes, we can reproduce the indirect impact. So now, actually we can we can um do this form of reconciliation. Then we could obviously compare that. But, the problem is the comparison has its counterfactual one, I can I I don't have I mean the the the other version it didn't happen in reality, so I don't really have anything to uh to compare to. I could take some orders out maybe and then I could maybe look at the execution algorithm I can remove my orders but then the impact is in there. So, it's it's very difficult thing to do, but I think the best thing we can do is see whether we will produce impact basically. Um specifically for things like the price prediction, um is there some way to quantify how much benefit you get from having the level three data versus just using the state of the order book cuz I can see into your model you put the state of the order book and you put the messages. Like if you just gave it that order book information you could probably make some prediction, do you did did you have some way of getting a handle on how much benefit you're getting from that? \n\n**0:27 Speaker B:**  Yes. We imagine this prediction. There's a pure prediction work, it's an older paper here, and we did compare it Yeah, in this paper here, we looked at adding the message data and we got some benefits that weren't huge, like some 20% better or so in terms of performance, give or take, if I recall correctly. It wasn't a huge increment there, but uh there was some additional information, so there was something orthogonal which wasn't captured there. As I mentioned before, um as a practitioner, you might get some way of obviously um We haven't the the limit order book obviously does not include those um trades here, so obviously I mean if you were were a practitioner you want to do full comparisons and you might always take the limit order book first, you add the trades and you use that as your benchmarks there. In the academic paper, we only compared about limit order books and then it is clear that the trade information is relevant. It is unclear how much it picked up beyond the trade information from deeper states from the limit order book. But I mentioned we do have this work which is coming out soon on this clustering, and there, we can see that we actually can identify by some relevant clusters where we can see different predictability on imbalance on the different clusters there, which is very exciting because obviously we do not know the identity, but if you have some idea of identifying roughly speaking statistically which order belongs to which group of market participants, then we might anticipate that maybe a fast market maker has an alpha which is over a shorter horizon than a institutional order, which is worked over hours, it might have a very slower uh predictability. And that's something this is something which is information you wouldn't be able to capture without this level three data, and we do see some additional information content. It's not a priori forecasting model of the types we we mentioned, but it's quite interesting there. It also relates there was some recent work by the Institute uh together with the Dutch regulator where they actually looked into different where they had the actual identity of the participants, and uh I wasn't involved in in in that work, uh but they saw interesting patterns there. And what we tried to say is actually on anonymous market data, could we maybe use some unsupervised learning to find at least groups, we can't tell for certain, but maybe there's a big likelihood that if a smaller order was canceled, just fractions after the price changed, that was maybe a more sophisticated market maker who puts this there rather than someone else and can that tell us something about the imbalance there? So, this is something we we use the information content of L3 and we see some interesting results there, even though it's not exactly these deep learning models, it's a very simple clustering. Thanks.\n\n**3:35 Speaker A:** So, question. In um in the training data, uh have you had any periods when the order book was one-sided, uh because at that point you would not have a well-defined mid, and uh did the replay reproduce uh any such uh one-sided moments?\n\n**3:54 Speaker B:** I mean, we didn't have I mean, there was some cleaning going on in the market data, so we always ensured that we had like, well-defined books, obviously, nothing I mean, you usually during liquid hours in this market, there's always a two-sided book, present, and there aren't any cross-markets. I mean, you could see those if you include or only include some auction periods or stuff, but we we make sure that we stay away from any auctions. We even usually stay away half hour from the opening just because there's this kind of liquidity information in the earlier parts of the market, so we looked more of the individual interval there. So, generally speaking, and I don't think we have seen any situations where there's a one-sided order book, but we haven't explicitly checked there. The only reason we would see it would be in those We didn't actually test for it, but you would obviously see in this uh in this um sorry, in this histograms here, if there was a one-sided book in the ask, obviously the midpoint price would be infinite, and it would completely mess up your distributions there. I mean, so we didn't explicitly check for it, but if it would have happened, you would see in those graphs there, uh yes. It only occurs very infrequently. If it happens on the ask, it would be infinite, and then you would see it, it would just mess up your entire I mean, if it would be maybe on the bid, and it would be a large sample, you might not be able to see it, but yes, so while we didn't explicitly check for it, you would pick it up in those plots here, so I can say that it didn't happen. And there are some questions from the way back, yeah, I will unmute maybe one by one, yeah. So, there is a question from Irena, yeah, please go ahead.\n\n**5:43 Speaker C:**  Uh uh Hi. Thank you for your talk. Uh, I have a question. What How did you define a tick? Is it a quote or a trade tick or both? And, what would be the expected time for a tick for Google to be present? Thank you.\n\n**5:59 Speaker B:** For this particular stocks, like Google, it would be presented Yes. So, um yes, so tick here, I mean sometimes we used different definitions of ticks, so in this first part when we speak about limit order books, oh sorry, when we speak about limit order books or add two data, we mainly mean by a tick, if there was any quote update within the levels we are interested in. So, if you look at a 10-level LOB, it would be any message within this 10 levels. If there was a message posted, if someone wanted to buy Google at 1 cent, then this wouldn't be showing up in our ticks here, whereas the MBO data here, the messages, those are every message happening there. So, so any any place or cancel of any type would be included, even if it goes outside of 10 levels. As I mentioned, we do we we do only look at messages when they are outside of 100 levels, we ignore them. Basically, so if it's 100 ticks off the midpoint price, we do ignore it, but in principle, it's every message, except when it's 100 ticks off the midpoint price, when we just filter it out. Basically, so this is the information. Now, the the time varies greatly from stocks is from stock to uh from instrument to instrument. It's roughly speaking uh let me just check that I get those numbers, um Right. I know for LSE I know by heart, it was like uh it's a smaller number order 10 of uh uh yeah, here this I think it's um order for Google is like order 10 milliseconds, if I'm not mistaken, the average tick to tick time. I will have to look it up again, but yes, it's just the time between every tick. I think it's 10 milliseconds on average there. Obviously, it varies between like between more active period and less active period, but on average, it's roughly around this.\n\n**7:53 Speaker C:** Okay. Thank you.\n\n**7:56 Speaker B:**  Yeah, we don't we don't have the summary of that uh oh, no, we actually have the oh, we do have it here. Yeah. Yes, yeah, here you can see here. That can be the minus uh uh we yes, uh yeah, that's what we here. Right. Yeah, yeah, thank you. It it's good also to compare this number with 10 milliseconds with the latency of the system as well, right? So. Yes, exactly. Obviously, I don't mistake those numbers by uh latency, it doesn't matter why, I mean, even though this might appear like largest numbers doesn't mean that you have to be fast, maybe to to execute. Yes, yeah, but yeah, this is uh much slower than the typical latency these participants have.\n\n**8:36 Speaker B:**  Okay, and there is a question from Shichuan Fan, yeah, please go ahead.\n\n**8:41 Speaker D:** Uh Hi. Uh Just wondering, do you in your order book uh training data, do you consider orders of a different timing force, like immediate or cancel, instead of a day order?\n\n**8:54 Speaker B:**  Yes, so we do have different uh different types of orders, uh are are correctly captured. Sometimes you can encode them in some extent, and there's obviously for some markets like the Nasdaq, there might be digital complications due to hidden liquidity, where there might be trades, and you might not see the actual liquidity there. If there's no hidden liquidity, you can actually replicate an IOC also by just putting in a placement and a cancellation immediately afterwards, so uh it will and the replay will give you the same results uh there. So, we worked before in this other paper I mentioned. We this paper here, we we used LSE data, and we effectively only work with placements and cancels and we will replicate the IOC by just the placement with a cancellation uh uh immediately like, call it so. It would be taken off immediately, whereas in in the Nasdaq data, we have proper message streams of the various order types. \n\n**9:51 Speaker D:**  Okay. Thanks.\n\n**9:55 Speaker B:** And there is a question from Carlos Vega, yeah, please go ahead. \n\n**9:58 Speaker E:** So, uh well, uh actually, two questions. The first one would be, uh I hope you can hear me. Like uh what you see as the limitations of this kind of modeling? Like, you could see cases where the same thing is traded across different venues, so you would have multiple order books to keep track of. You can see cases where two stocks are correlated, let's say on the same sector, uh uh and even for example, looking ahead into other markets like stocks and futures, and corresponding options and so on. So, what would you see as the limitations of this? And and then the other one was, if you are running this in kind of a live environment, how much history would you be able to be able to keep in the model because you're you're not retraining so how many events would you keep would you be able to to to kind of feed it as the context to predict the next move? Thank you. \n\n**10:47 Speaker B:**  Yes. So, firstly, obviously yes, yeah, there are different sources of liquidity, especially in equity markets, it can be quite fragmented. This is all univariate models and basically on the primary being at LSE or being at uh Nasdaq, obviously there are other sources of liquidity, MTFs, dark pools and we just can't include this information. Our main hope is that the liquidity on the primary gives us a good representation of the overall liquidity, but that's obviously a limitation which is um which we just have to uh take into account. You could obviously try to look into a consolidated book um for example, but um yeah, we think that this type of model on the messages, you are better off looking at the individual uh exchange, rather than looking at the consolidated book, but obviously there's information coming in from other sources, and it's hard to capture it. There are some cases, and obviously the same is true for correlated instruments, yes, there could be correlated instruments, and maybe a trade in one might be influencing it, I mean, we I could just say that you might see the maybe I don't see the other book, but the market maker who sees it might place an order and I just pick up this other order then, so there's some way in which this information maybe ripples through. To me, that I know from practice, there are some people some cases where people explicitly look at this. For example, if you have like options, then if you have a because you have put call parity, you can immediately unify a putting the call limit order book into one because there's a direct matching, that's obviously like ideal correlation you have here because there's a mathematical relationship. So, you you they are situations when people build together books to look at the joint liquidity, but you wouldn't do that for like weekly correlated or correlated instruments which are not like identically matchable like Amazon and Google. You wouldn't try to join limit order books. Best people do is look at consolidated books, but we don't use we just use the primary here which hopefully you might argue is the kind of uh like, source of liquidity and price formation, and that's how the model works well there, but yes, there's potential scopes of extending it, and looking into a uh consolidated books or other sources. And people do that in practice, but um yeah, I haven't seen any good deep learning models which do limit order books in the cross section there. People do cross-sectional modeling, and this information will it ripples through a bit slower so you might have a different type of model for this effect than you have for the actual limit order books. Most of the academic limit order book literature is all univariate.\n\n**12:50 Speaker E:** Yes. And then the second one was on the history. So, if you're running on a live environment, how much uh would would be available in the model, let's say, the last uh 5 minutes of messages would be would would be as much as context uh as the model can take, like the number of tokens you can keep in your in your sessions, right?\n\n**13:10 Speaker B:** Yeah. I mean, just just to clarify I mean this is all academic work from the University of Oxford, so it's not running in live environments. But obviously, we want to make sure that the academic work is of relevance and maybe useful. So, I mean this ones here looked 500 ticks into the future, as I mentioned, so 100 ticks can be anything from uh 10 seconds to 30 seconds or roughly speaking, depending on this depends on the instrument as well, so so you're looking like five times that, so you're looking at uh order minute, single minute, maybe 5 minutes you're again, depending on the liquidity, that's what you have as a look back, and you try to use that to make forecasts. That's generally good look back for this type of information you are looking for. Anything beyond that might be a signal of a different nature. You might argue that maybe the order book effects decay away, and then it's maybe more the cross-sectional information which is maybe relevant at this frequencies. You might argue this the the the any information from the univariate limit order book is the one which is up to way the fastest, followed up by some cross sectional effects which relate different instruments, followed up by some other effects. So, so this type of information I think is a reasonable look back to look at, I wouldn't expect those types of signals to have to be used for forecast ability beyond like, 3 to 5 minutes. You would look at other type of features and other type of data, and you would definitely look into the cross section at this longer time period, so it it is I think it is relevant for practice because for the type of data you look at, you don't expect it to have forecast ability much beyond this, so it's a reasonable input really. This is uh Yeah, I was thinking more like, let's say a dialogue, uh the model uh was loaded with uh a number of messages, it replies with a prediction, uh now you kind of reply back with the real realization, then another prediction, and so on, and and the model accumulates this state, and and LLMs typically have a capacity on how long the dialogue can be, and and that's that was what I was after like how long could this capacity be in terms of ticks or messages, or for minutes, but thank you very much.\n\n**15:22 Speaker B:**  Yes, yeah, but I was just saying I mean I mean yeah, maybe now it's a little bit better. Just quick comment, first of all, there are different ways of training those models, there are things like teacher training and things to like, for example, am I training it, if I predicted the next message, and I want to roll it forward, I can then actually replace it with the actual message, so I'm only ever in training going one step into the future. In training it's not rolling 100 steps, it's only rolling one step at a time into the future. I then I predict the message, say, okay, how was that, and now I just actually when I then roll forward, I used the actual correct message, then, if I train it on historical data, basically, so this is how this is called teacher uh training, this is our teacher forcing, which is how it is done, uh basically. And then in then, you might argue obviously how long can you run it forward. We run it forward for 100 ticks, and we see some good correlation. What happens if you run it forward for an entire day, it's uh it's questionable how well it works. Obviously, it doesn't mean that the model has to have a look back of an entire day, it only has a look back of 500. It just keeps on rolling this forward, but obviously, if you keep on running it indefinitely, it eventually will diverge from from from the historical data, and how realistic it is, that we have some ongoing work which I tried to mention where this current model, the loss function, only looks at one step ahead. And, you can then have another model which then evaluates all the sequences of messages, say, 100 500 ticks into the future, and compares them to actual ones and evaluate the entire sequence at a time. And, you can then this is another form of loss function, there, so this is something we are doing right now, is called actor critic. And this is ongoing work, there, which will hopefully help us to get better long-term sequences which is what we eventually want, if you say you wanted to use that for for simulation of say a whole day of trading, or something like that. So, yeah, that's something ongoing, it's a good question indeed.\n\n**17:21 Speaker B:**  A question in rune?\n\n**17:25 Speaker F:** Hi. Thanks, Stefan. Um, I don't know if this question is applicable now that you said that you haven't tried this in live trading, but, have you tested the approach in a situation in which the 500 messages that you feed the model with include the time in which um some relevant significant announcement happens, like, you know, quarterly uh results, or a monetary policy announcement? And, if so, what did you observe?\n\n**17:50 Speaker B:** We haven't specifically dialed down onto those things. One of these ongoing ones is where we want to check because, obviously, this model seems to be doing an okay job in normal situations, but at some point, obviously, you might run out of the distributions of or out of the comfort zone in simpler words, uh where where the model is used to. So, that's something we are looking at right now. For example, could you reproduce the typical shape of a flash crash if you just inserted a very, very big order? We haven't tested those things. It it might not be that the model if it has never seen that, it just it's a kind of that it it's not performing so well in those tails there, but it's obviously something we can test, and then we can look at other things like specifically around announcements or other things. We haven't done that in this specific one. In some other works, like you know, in other context, we did look at, say, some of our momentum work with attention, we looked at what happens during COVID, what happens around Brexit vote, et cetera, et cetera. We haven't it for done it for for this specific project, but that's obviously something once we want to gain more and more comfort into the model, which is what we will be doing as well, to understand where are its limitations, when do we uh push it outside this comfort zone.\n\n**19:11 Speaker F:**  Okay.\n\n**19:13 Speaker B:**  Any further questions? \n\n**19:23 Speaker G:** Um sorry, did you see the simulator learning about uh heat hidden, sorry, lit hidden liquidity, so like, interest rate liquidity, and like, icebergs reloading?\n\n**19:42 Speaker B:** Well, it it it uses the available messages, and the messages obviously have trades where if it and it gets the limit order book as comparison, and it's maybe it is one of the reasons why you might argue that maybe it is also useful to include this order book there, because obviously it's easier for the model to figure out when liquidity is hidden, if it sees the limit order book, and it sees the trade happening, and the the passive side wasn't there, then it must be hidden, and it helps the model, obviously, to see that. So, yes, it does see all the trades, and it it has a way of figuring it out effectively. So, and uh yeah, this it's that's a that's a right way, but yeah, we can only model the I mean, we can only model the sequence of messages we can see actually. \n\n**20:28 Speaker G:** Yes. Yeah, in regarding the tokenization, uh I mean, I think that one of the main challenges of applying uh machine learning models in finance is the the suppose to like, images or text, the dimensions are not comparable. Because, for example, in an image like like let's say the horizontal and vertical dimensions are like more or less like the same, right? But, in finance, for example, we have in one uh we have the price, and in the other one, we have the time difference. Um yeah, how how do we get around that uh the fact that like features are very different in like, different dimensions, but the architectures treat all the dimensions the same. Like, for example, if you apply a convolutional layer with a kernel like I mean, like this filter, like the kernel is like doing the same in all the dimensions. Basically, of course, with different parameters, but so how do you get around this?\n\n**21:23 Speaker B:** Yes. I mean, there are different things you have to obviously at the beginning, when I spoke about deepLOB, we have some convolutional filters and there we have actual numerical information, and you can make sense of those things. Here, we do this tokenization is mainly to quantify uh uh no, not quantify, quantize in a sense, the vocabulary, because then we can use it like as if it was a classification task, we can have a cross entropy, we get softmax probability, and we can sample. We couldn't do that if it was a continuous variable. So, now we have like a unique ID for every possible tokens, so every possible message is just a combine a unique combination of tokens, and it's a nice countable set with its fixed length set, and we can easily deal with it. That's why we do this tokenization. It's it's mainly the main reason is so we can use cross entropy loss, that's the real reason why we do that. Yeah. Everything else you can deal with some if it was just a normalization or so, we could find a way of normalizing it also. If you obviously if you don't want to share um you don't have to share uh time or price uh filters there. You you know, you wouldn't be sharing the filter over variables of a different type. It wouldn't make sense, basically. So, but you could still do relevant normalization. So, I would say, if you're just after normalization and combining features, you don't really need this tokenization. The tokenization is the reason why we have it here is only to have a finite number of a fixed length of possible uh tokens over which we can use the cross entropy loss. That's why we ended up with it here. Everything else, you can really do with some smart normalization which is what we do here in this limit order book, and when we look at these images, before like, those ones, this is all just nicely rescaled, and for example, we we used convolutional filters. But, the convolutional filter stretches the entire image, and it just runs like this. You wouldn't be able to have a convolutional filter used like this, because this is price this is side kind of use the same pattern, that's something different things there. Yes, you have to worry about those things. In any case, but you don't need tokenization for that. We didn't use it here for this thing.\n\n**23:39 Speaker F:** Thank you.\n\n**23:42 Speaker B:**  Right. Um on slide 20, you talked about the way you encoded different um There's a tiny number here, so good for your eyesight. After you click through it, talked about uh cancellations there, um and the way that you encoded them. \n\n**23:58 Speaker B:** Yeah. Um so so for the the buys here, um o or for the um that the place instructions, you know that they are always on the back of the book, um well, for the cancellations, this could happen anywhere through the Q. So, is it possible we're losing some information in that example, because the encoding doesn't tell you where in the queue that cancellation was? \n\n**24:26 Speaker B:**  Perhaps. It's instantaneous, why not? But, obviously, the sequence of messages uh in has this information inside. I mean, because the earlier message has the quantity of the order. We played this information is there. It just can't do exceed in the snapshot, basically. So, the MBO data is all the I mean, all the information is in here already, it's just this is the kind of incremental pieces of information. But, if you aggregate, you would know this information. You have to just replay it to look. This and the information is there, it's not lost. Yeah. So, even in this statement, yeah, all we do is add a bit more information to this one, because this is just to But then it was a model to make any sense out of this information, or else, it would be fair, this only makes sense if you're practice correctly. You see, this also only makes sense if you have the history. You would have to see where in the queue it was actually placed with a relevant ID here, to be able to know where it was there. But, information content wise, the MBO data has everything, after all, that's the data you get sent from the exchange, is all you will ever going to see. Yeah, so that's all the information is in there. I can't get any richer than that. The question is only what kind of states do we build out of it. And obviously, all do we have something which potentially the information is there, but I might have to go further enough into the history. I'll do it. So, we do have some tricks also. I I'll tell you all the tricks, but there are some tricks here in this thing. So, this thing, let's just say it's a start limit order book and we do know all the IDs in this start limit order book, then, if it comes up with some cancellation, then it will look whether it was there before, and it can also go it has to start the limit order book at the beginning as well and can see whether it was there. Right. So, that has some extra built-in features there. But, it's mainly about the history, so you you you're not losing any information. I hope that's clear. It's just it's easier if you could see this information instantaneously, without having to look something up. The queue position is implicitly encoded, but it's not explicitly seen in the message. But, the model has enough information to learn it, basically, this is the main thing. You could try to add it explicitly, but then the question becomes, what else do you want to add? Is it just the position in the queue which is relevant? Is it maybe relevant how many orders there are? You can think about all kinds of variables you could add, and this is again handcrafted feature engineering, so we just try to give it the raw messages and hope that they learn those features by itself. The only question is, do we remove some relevant information, so it's not able to learn it because it "

