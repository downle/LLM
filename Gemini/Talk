"0:00 Speaker A: Hi, everybody. Welcome to the Buford One Speaker Series. Um, today we have a talk by Leif. Leif Anderson is the global co-head of the quantitative strategies and data group at Bank of America and is an adjunct professor at NYU's Courant Institute of Mathematical Sciences and at CMU's Tepper School of Business. He was the co-recipient of Risk Magazine 2001 and 2018 Quant of the Year Award and the recipient of the IAQF North-Side 2024 Financial Engineer of the Year Award by the International Association for Quantitative Finance. And, he has worked for 30 years as a quantitative researcher in the global markets area. He has authored influential research papers and books in all areas of quantitative finance and is an associate editor of the Journal of Computational Finance and Mathematical Finance. Um, this talk is being attended by clients, um, so I'm going to just going to read out the compliance statement. This talk will be open to a number of Buford clients who will join us online. Given that there are individuals present from different organizations, please do not share or discuss any sensitive or confidential information or material that could result in a conflict of interest or any competition legal concerns. Leif, over to you. \n0:58 Speaker B: Thank you very much. Um, so today's topic is going to be about American option pricing. A very old topic, uh, but there's still some life in it. Um, the most important part of this slide is for me to remember to say that this is not all my work. It is based on work with Rob Avery, Mark Lei, and Dimitry Yurk again. The first two are at Bank of America and the third person was at Bank of America. So, it is definitely a Bank of Amer- America effort that I'm going to talk about. So let's see if I can advance this. What do I point at? I don't know. Hm, maybe uh, let me, hm, okay. This is a fair course. Yeah. You can just press the No, nope. Maybe I just do this slide here. Okay. Um, so slight technical difficulty in advancing. There we go. Okay. Let's see if it works. It's working. Yep. It's working. Yeah. Okay. So this will be a financial engineering talk. For those that are here to see something artificial intelligence, I suggest you just leave the room uh, because this is what this is going to be about. And, it's going to be on probably the most financially engineering issue problem of old time, namely American options. Um, this is a topic that has occupied engineers in the financial space for five decades, and it's still not quite settled, especially as it pertains to how to do these things efficiently. Um, so this presentation here is going to be high-level. It's going to be a little rushed. I have too many slides. I'm normally an advocate of using very few slides. But, this particular topic, it cannot be avoided simply because, as it turns out, the American option problem is not one that can be attacked with a single method. Circumstances will dictate that you need multiple me- methods at least if you want to do the most efficient and, you know, most performant, smoothest, and most convergent methods. Um, and then you just have to to bite the bullet and realize that there are a variety of branches you have to take in your code and in your thinking. So, here is just the uh, we'll start with a warm-up exercise about the Bermuda option. So, here I'm looking at a Bermuda option. It it's process value is going to be V sub B. That's the fair value of the Bermuda, and this will be exercisable on a discrete schedule. So, there's going to be an exercise point uh, on this, and if you own one of these things, then you have the right, but not the obligation to exercise it at any point of this discrete schedule. So, you will have N exercise opportunities. And, depending on what contract it is, there will be a specification of what you'll get if you exercise. So, E sub B here is the exercise value. This is what you get if you exercise. You don't have to exercise, but when you do, that's what you're going to get. And, if you have some exercise strategy here, nu, that must take values in this discrete schedule in mind. So, you may come up with something, then I can associate that exercise strategy with a present value, which is simply just the discounted value of the payout at the time nu that you get it. Nu here being a stochastic variable. You don't quite know when you will exercise. You have a strategy in mind, but it's probably stochastic. And, here beta is the money market account, so just a discounted, expected discounted value in the risk-neutral measure. That should be uh, uh, old hat for most of you on in here and on the phone. Um, now, if we assume that the holder will will exercise to maximize the value, then the Bermuda option value is simply the supremum of all these individual uh, the values you get for com- for the universe of exercise strategies. So, you pick the best strategy, B star, is the optimal exercise rule. So, that's how we're going to that's going to be the American, the the Bermuda option value. Of course, if the holder doesn't ex- doesn't follow the optimal exercise rule, the guy on the other side can get a free the holder. So, this is sort of the arbitrage-free value of the Bermuda option value. So, how to characterize it? So, we have already defined something called the exercise value, E sub B at time T i. E sub B is the exercise value. Now, we're going to define the holding value, that is simply the value of the Bermuda option if you choose not to exercise. So, the holding value will be the present value of the Bermuda value one period ahead, because you know you're not going to exercise it now. So, what you're looking at is just what's in the future. So, now you have two things: the exercise value and the holding value. And, then the Bellman backwards principle basically just says, at current time T sub i, the the value of the Bermuda is going to be the maximum of the holding value and the exercise value. And, if I stop in my definition of the holding value in here, you'll get equation one. And, this is an equation that a lot of people have stared at, and especially if you're a data scientist, you'll look at a value like this and and and notice sort of the recursive nature of it, the value today depends on something in the future, and it has a a reward function. So, if you are in the machine learning space, this to me this to you would look like a reinforcement learning problem, and you would then attack this with a neural network to estimate the holding value for Monte Carlo samples. So, that's kind of the a lot of,  you see, this sort of reason work on on this problem is data scientists jumping into it and saying, let's use neural networks. Let's use this technology for this problem. But, to me, that's not such an interesting avenue. There's a a version of this, there's a finance-oriented Monte Carlo algorithm, the Longstaff-Schwartz algorithm, that works very similarly, that's it uses a polynomial regression, and that particular approach for all practical purposes that I've seen so far is just as performant as a neural network and is definitely me- also simpler and has, uh, in many cases, you can set them up to be better and easier to explain. But, anyway, this particular problem, the Monte Carlo approach is not one that I'm going to spend too much time on. Just know that, you know, there is a branch, a relatively recent branch of work in this space that are looking at applying AI type methods to it. And, that's the closest I'll get to AI in this talk. Just so you can do it, but I wouldn't really recommend it, except for very complicated problems. So, the American option is just the limiting case of the Bermuda option when the exercise becomes continuous. So, you can exercise anytime you want, not just on a discrete problem, so in a on a discrete timeline. You could say, well, I could just price everything as a Bermuda, but just let the limit go to the spacing go to zero. Obviously, a very, very slow way of doing it, and it's also only linearly convergent approximation. So, uh, I would not recommend that you do this. You want to do other things as we'll get to. But, you could, if you have a method for Bermudas, and you put in lots and lots and lots of exercise points, then, you will converge to American just very slowly and a co- the better it's going to be brutal. As you say in the interest rate space, Bermuda options are far more common than American options. However, in the equity space, which is what I'm going to look at, is the other way around. A significant majority of exchange-traded options are in fact American. So, in the equity space, volumes are high, model dimension is low, and speed precision is of important. So, wouldn't suggest a Monte Carlo approach for sure for this. That would be an overkill, and it would be too slow. First part: integral methods. Here are some references uh, that you can look at if you want to dive into it uh, later on. So, there are many, many, many people that have written in this. Of course, I have listed the talks that and and papers that I've been involved with because I'm going to le- lean on them, but just open any one of these, uh, and you will see lots and lots of other uh, references in there. So, okay. So, in the equity world, an advantage of the equity world besides being low-dimensional is that it's still actually very relevant to look at Black-Scholes like dynamics, because quotation practices and and implied vol calculation and interpolation and the Americanization, lots of procedures, require you to price American options in Black-Scholes like dynamics. So, let's start with that. And, here in the middle of the page, we have the Black-Scholes dynamic in the risk-neutral measure with an interest rate of r and a dividend yield of q, and a volatility of sigma. The approach I'm going to talk about, it can also handle uh, time dependence, and it can also handle negative rates and dividends, and you can see a reference that that shows how to modify the algorithms for that case. I'm not going to do that, just because it gets messier. And, then I have I'm looking at a particular payout now. So, that's E sub B that I looked at, which was the payout. I'm going to look at a particular uh, contract now, namely the American put option. And, it just pays the maximum of the strike minus the stock price at the time that you're exercise. So, if you can do the put, you can do the call by put-call symmetry. So, that's optimal exercise strategy I discussed earlier. For this particular case, it has a very simple form. It's known that you exercise when you are below some barrier, A star T sub T here. So, it depends on the maturity, which is why I put an index on the T a T index on it. But, there is some barrier that if you go below it, then you should exercise. And, this is what it looks like. There's a you see this blue thing separates the universe into a hold region and an exercise region. Not much to say about it, other than it kind of looks like this. uh, with that, and maybe notice that as you get close to the maturity, T, there's a very rapid variation. It kind of goes sort of slowly, and then suddenly, the boundary picks up and becomes vertical. So, there's a very rapid speed uh, as you get closer and closer to maturity. So, that's something that you need to think about in a numerical method, and it's something that reference to spends a lot of time on dissecting and understanding. Um, so what so beyond having this exercise boundary, um, what else can we say? Well, what I'm going to do here is I'm going to flip time around, so instead of working with calendar time, I'm going to work with time to maturity. So, capital T minus little t, how much is left. So, we're going to run time the other way so to speak. And, in that world, we know that the uh, that the American put price satisfies a um a Black-Scholes equation, if you're in the uh, in the continuation region where you're still holding onto to the option. And, then there is a couple of conditions. There are actually many conditions, five, six of them, that characterize the boundary between exercise and non-exercise. One is the value match condition, that just says when you hit the boundary, you're going to get the payout, so strike minus uh, the uh, the value of the of the of the stock price which is equal to the boundary when you hit it. And, then there's smooth pasting, which is that you paste smoothly onto onto the put option ramp. And, there are a few more that I didn't bother writing down uh, that talk about what the gamma is like, what the data is like, and and so forth, but these are the most well-known. So, the list of these is the characterization of what the boundary uh, must satisfy. And, then a cool result that dates back to actually quite a while is that it shows that the solution to this PDE and the solution to the American option uh, option price when you're in the holding region can be written as the European price plus some simple integrals. Which is a remarkable result. Instead of having a PDE to solve, now you get just an integral. It's just an integral in time, and it just what does say these integrals involve just some cumulative Gaussians. But, they also involve the boundary itself. So, as a practical matter, evaluating this simple-looking formula requires one to establish this boundary in the first place. So, it's a cool equation, but it has uh, and it has from a numerical perspective, your eyes pop open when you see a PDE turned into an integral because integrals from a numerical perspective are far better to work with than PDEs. PDEs, if they can be discretized, have all kinds of problems with their convergence order with rounding error and so forth and tend to be slow, whereas an integral can be attacked by quadrature methods, and those methods can be made almost, you know, arbitrarily fast and and speedy and converge really, really fast and so forth. So, this looks like like a promising avenue. The only problem is we need to find that boundary. And, one thing you can do is you can take the equation and say, well, it needs to hold on the boundary itself, because it holds when you are in the holding region, but also right at the boundary. So, then, you can stop in the boundary everywhere we said S and you get an equation for the boundary. So, that's great. Now, you have an equation for the boundary. It's a little bit of a complicated equation because on the left-hand side, you have the boundary evaluated as a function of time, and on the right-hand side, you have integrals uh, over the boundary. So, so these you have like a function, and then you have a functional on the other side. So, this is one equation you can do. You can also use the smooth pasting equation, or you can use expressions for gamma. You can come up with a large number of equivalent characterizations of the boundary. They're equivalent, but they may have different numerical properties. And, you can look at the re- reference number two if you want to see an analysis of some of the different approximations. Ultimately, all of these approaches will lead to equations for the boundary. This form here, seven, that has a numerator and a denominator that are both are functionals. So, they involved integrals over the life of the option of the boundary, and how do we solve a system like this? This is like an integral integral equation. Uh, as it turns out, there are many ways of doing it. The fastest, we think, is to use a fixed point method, where you guess or you start with a really good guess for the boundary, and then you plug it in on the right-hand side, and then out will pop a new recommendation for what the boundary should be. And, then you pop that back in, and then you iterate on this. This particular method is is is a collocation method that's proposed in two. It does require some interpolation and on the boundary, you need to trans- introduce a number of transforms. But, pretty much, that's how it operates. You guess at the boundary, and then you plug it in and see what you get back for the boundary, and then you iterate. Normally, two, three iterations is enough for even a high-precision application, and once you have found the the boundary by this iteration, then you go back to this which is just an integral. You can do that by your favorite Gauss-Legendre or any kind of or a Chebyshev, any kind of quadrature rule you like. Pros and cons for this method: spectral convergence, which means spectral convergence to me means that if you double the effort, you get twice as many significant digits of precision. There's very di- different from the type of convergence you will see in a in a finite difference grid or something. When you double the effort, then you get because the error is just cut in half or cut divided by four, here it works much, much faster. Double the effort, twice as many significant digits. I accurate. So, it's a completely different ballgame once you're in there. Exceptionally high speed and precision. So, we we ran some tests, just to see our latest implementation. So, if you want 10 to the minus seventh precision on a modern computer, and not a an an average modern computer, not a supercomputer, a single PC that you can buy in a store, um, it's about we the speed is something like 125,000 options per second. If you want to compare that to a binomial tree, binomial trees will have a very, very tough time hitting 10 to the minus seven because in part of the way they are constructed. But, let's say I think you can do much less than one option per second at this precision, just because you get 10 to the minus third, 10 to the minus fourth, if you're lucky, in a binomial tree, and then to get three orders more of precision, there's no way you can do it, so basically and of course, if I need less precision, I can just the number goes up correspondingly. If you just want 10 to if you want 10 to the minus 10, then, we can do something like 10,000 options. So, a lot of precision. Do you need it? Well, depends what you are what you're into. Any routine that is inserted in some kind of loop, a calibration loop, should be accurate. Um, but, anyway, uh, you can take those numbers. If you want less precision, then, you can move them all up. What are the drawbacks here? Difficulties with discrete dividends, especially for American calls, um, and then it requires close-form European option price formula. So, if you have some kind of for instance, a local volatility model, this is going to be difficult for you, because it does ultimately require a European option price formula embedded in in in everything. Good. Uh, so so let me just talk a little bit about dividends. I said that here, it has difficulty with discrete dividends. What is the problem with discrete dividends? Now, I'm going to take my process, and I'm going to modify it. Where I take what was before was q, which was a dividend yield. I'm going to stop in instead jumping dividends at some schedule. Every time you cross over that those dates, then the stock price will jump down by a certain amount, d i, that can be a function of the stock price just before the dividends. And, there is only one case of this where you can han- that can be handled with this integral method, and that is when the dividends are proportional. So, they're this form that they're basically a certain fraction of of of the stock price, where the fraction must be between, you know, 0 and 1. So, you don't want to pay more than the stock price, but you can pay half of it or 1% of it or something like that. If you have that case, it is possible to extend everything. I'm not going to go through it, but in this case you will see that, this since the uh, the put, you will never exercise it right before a dividend is paid. So, the exercise boundary starts looking a little different. It gets driven into zero right before every day dividends. So, you get this kind of of look to your uh, to your exercise boundary. And, what you need to do to the method is just split the domain into one, two, three pieces, and then you can basically repeat the method. You integrate over these pieces one by one, and then you run the methods from the back. And, you can see that in one of the references, how to do that, is not difficult. So, you can handle it, and that will be on average, the speed of this, if you have In this case, you have two proportional dividends, then, you have three regions, regions. So, you're going to be about three times slower, but a little better than that because these things here, these are very smooth and easy to handle. It's only the last one that's a little trickier because it has this, you know, as you get get closer to the bound to uh, to maturity, then the boundary becomes more unruly. So, but basically, if you have four dividends, then take the number of three two dividends, take the uh, the numbers that I showed you before and multiply everything by say, two and a half. And, you're still going to have a very, very performant method. So, now that was for the put, for the American call with these types of discrete dividends, the wheels kind of fall off these methods. And, that's because the boundary for an American, the exercise boundary, is going to look very strange simply because, as it turns out, the only possible exer- exercise dates for a call are just before each dividend. Unless, you have a borrow cost, which is a different story. But, if you don't have a borrow cost, then that's the those are the only dates that you could consider. And, sometimes, you wouldn't even exercise on any of those dates. So, the call is very sparse in its exercise opportunities. There's no boundary. There's just a set of points, so so the exercise boundary now gets degenerate, and there's nothing to integrate along anymore. So, for this case, you cannot use the integral methods, and you are going to do something different. And, that's what I'm going to talk about next. So, if you have now, you have discrete dividends. So, how you going to handle those? So, I'm going to go back to this is the process here. I have a still geometric Brownian motion on the drift and the volatility term, and I have this series of dividends that that jumped. Before, I just looked at the proportional case. I'm going to extend that to be more ambitious now. So, I'm going to let this be a more general function, so the dividends can be a more general function of the stock price. For instance, you can use this this, uh, mixed dividend model, which is it has a proportional part and a fixed part. So, the fixed point part is some dividend that you know for sure will be paid, then maybe that's a little bit of extra that depends on how well the stock price is doing. So, if you have very sticky dividends, you will have a big fixed component here. This model is very popular. Very often, you will just get rid of entirely of the proportional piece and just look at cash dividends. It's popular, but it has some It's unrealistic, so while I'm added here, let's talk about what I like to build an approach that can handle also more elegant and better uh, specifications of the uh, of the dividends. For instance, in that model, the mixed dividend, if the stock price is too low, at some point, then you have a dividend. You're going to cross through zero. Let's say this cash piece is is nonzero, then, uh, and you in the model, it it maybe the stock price is like worth $1, and this dividend is supposed to be $2, and then you're going to get into zero. So, not super great. Um, still a very popular model, and everything I'm talking about here will be able to accommodate the model. But, I like maybe this last year rational dividend policies. And, basically, these are just specification where you ensure that the dividend is never bigger than the stock price, so when a dividend is paid, it is supposed to be less than the stock price. So, you don't pay yourself out of house and home or something like that. So, here are the uh, four models that were listed in there. Model 1 is the proportional uh, dividend model. Model 2 is similar. Model 4, the last one, is the is the mixed of cash and uh, and proportional. So, it has the unfortunate fact that it crosses over this 45Â° line. So, for sufficiently small stock prices, the dividend is bigger than the stock price itself. But, then, there's a variety of things you can do to prevent that. One is to think it down at some point. That is what model 2 does. At some point, it's going to say, no, I can't pay that that much in dividend, so I'm going to, you know, start paying less, and I'm paying that sort of proportional only. And, another model would just say, stop. I'm not paying dividends below a certain uh, threshold, because then, I'm in a bad shape, and I'm not going to pay any dividends, so all these dividends, and you can invent many more, can be accommodated. So, an interesting thing is, of course, these things themselves, the dividends are now except for model 1 and model 4 here, they are nonlinear in the underlying. So, that means that the expected dividend payment, and therefore, also the forward price of the stock, must be dependent on volatility. So, that's annoying, but it's just it's a price you have to live with if you want a model for dividend payment behavior that doesn't say that I'll pay this amount, no matter if I have the money or not, and and build a model that also allows for negative stock prices and so forth. So, you have to choose your poison. Doesn't really matter, however, what you pick because the approach I'm going to show will accommodate everything, but it will also allow you to switch into a dividend model that isn't completely crazy if you have, uh, say, a longer-dated, or maybe a relatively low uh, low stock price relative to the projected dividends. Yes. Okay. So, when you're in this kind of setting, um so basically what we have here is we have a geometric Brownian motion, and it kind of in between things, and then, if you have some jumps. So, it's it's quite natural in this setting to introduce a um to start looking at sort of densities. So, that's what I'm going to do here. I'll stay in this rational dividend policy class simply because, and then I can define x as being the log of the stock price because I know the stock will not go below zero. If you look at the uh, at the reference, you'll see that's not a prerequisite. You can you can get around this without any any pain. And, now, I'm going to say that I actually, I know what the stock price is right after a dividend payment at T i. So, if I know that, then I'm going to ask, what can I say about the uh, the probability density of the uh, the log stock price one period ahead, but just inside. So, we are right after dividend at T i, and I want to say something about the transition density just before the next dividend is space. So, I stays sort of inside. Uh, there are no jumps in this little interval, and in that case, it's very easy to show that then the that the density of x um, of the of the the value of uh, of of x right before the next dividend date, given that you know where you are right opposite of the previous um, is just Gaussian with some with some means and some uh, and some volatility that is easy to compute straight off the uh, uh, the geometric Brownian motion. So, that's given on the slides, so shouldn't be surprising. In between dividends when nothing is jumping, everything is log normal, so the log stock price is therefore normal, or Gaussian. Okay. So, now we know that we have a density in between and I'm going to start as a warm-up here, looking at European call options with the aim of getting to the uh to the American call. And then, I'm just going to show the the European call is just the expected value of the call payout at maturity. And then, if I want to do the expectation of some little t, this is conditional on whatever the stock price is at time little t. So, I get a an expression here that depends on my stock price and my little t, and it's just an expectation. Now, I want a way to calculate that. In particular, I want to be nicer to be able to calculate it at time zero so I know what the call price is, well, right now. And notice that I removed discounting here, just for simplicity. And this kind of so since this is an expectation, it means that what I can do is by sort of iterated expectations, I can say the call price at any time little t as if I know what the call price is at sometime after little t and before maturity, the call price is just an expectation. So, we have like a Martingale condition here because I removed discounting. And I can write that out and saying, okay. So, this expectation here, so, in between dividend payments, I can use this application here and just write that expectation as an integral. So, this expectation here in log space just becomes an integral of this type. I take the the call price one period ahead but just before the dividend is paid, and then I just integrated it against the density which is Gaussian and that gives me the uh call the European call option price in log coordinates at time t i plus. So now I have a way of relating call prices inside uh the dividend payments. And then, to get across a dividend payment, I use a jump condition. And that just says that call price just before a dividend is paid must equal the jump the the call price just after. But, you have to adjust. You know that this stock is going to go down by the dividend, so you just reduce the the stock price amount on the right-hand side, and then you end up with a jump condition at that. And that relates sort of the little t here that t i + 1 minus which is just before the dividend to uh to the call price just after. And then, I can take my expression 14 here and substitute in for the the c part here uh this jump condition, and now I get a way of relating prices at x dividend dates. So, t i + and t i + 1 plus and t i +. So, getting this notation here is quite an ordeal but I think I got it more or less right. So, now I have a relationship between call option prices at x dividend dates. And, it sort of sits there as a nice integral. The very last period um this call option price here will be just be the value at maturity. So, I can actually execute that out and get a formula. So, we start these integrals start with a formula. And then, you have to sort of integrate backward. So, that's kind of the scheme that is being I'm I'm I'm pitch I'm pitching here, setting up that you start with at the last dividend, compute an integral based on the payout and then you roll your way backwards in a series of integrals. So, you can think of it as a little bit like a finite difference grid, but everything is now in integral space. So, okay. So, that's the idea. Iterated backwards through these integrals. But, then, you have to evaluate these integrals numerically, and that's what I'm so If I integral number 16 here I condensed the whole thing into a into a function f, that's just written here. And then, I get So, I basically have to compute things like this. So, a very simple function, and then I have to integrate that against the density. How would I do that? You you would use some kind of to get a performance method, you would use a quadrature scheme that is performant. So, not anything like uh nothing primitive, nothing like uh like like you'll see in in in beginner's textbooks, but you'd write Find some really well performing quadrature rules. And then, you're going to pick some source points you have to evaluate. Then, you pick some values of t, and then there are some weights. And then, all quadrature schemes will look something like this. It's a bunch of weights times these functions here evaluated in a set of points. Now, the choice of those points and the exact value of the weights, that's sort of all in the secrets of So, some quadrature rules are much much better than others. Some are very simple, some are very cool and sophisticated, but they all kind of look look like this. Um, and then you you can do this. So, now you can do this for any value of x, and now if you step back you will need this for multiple x's, because the previous slice, as you roll through this will need a something similar, and we will therefore need for this to be evaluated at a bunch of x's. Ultimately, you end up with sort of some slices with different points that you're looping backwards. So, it looks something like this. You have some sources that you set up, and then you have some targets, and in every instance for every target you have to do these weighted sums. So, if you look at this, if you sort of count the number of connections, you'll see this is a quadratic algorithm because for every target you end up with something that is order m here, the number of sources. And you have to do that for every target, so you end up with an order m squared algorithm if you do it naively. So, it's a quadratic workload of this type of algorithm that we don't want. And, of course, that You should think that if you What we're effectively doing here is some type of convolution. So, you should think ah maybe we can move this into convolution space and make use some kind of fast Fourier transform algorithm. And that's true. The only problem is here is that because we are using a good quadrature rule, these points are not equidistantly spaced, that we are looking at. They're far from equidistantly spaced, and that would be terrible. So, that means that a regular fast Fourier transform which requires equidistant spacing cannot be used. It will lead to a terrible scheme, and you'll see a lot of FFT type schemes in the in the literature. They actually are all pretty bad. They look good, but once you test them you see that they're slow and flaky, and don't really work that well. You can use a non-uniform FFT and there's a reference to uh some work that I and Mark did on this complicated to say the least non-uniform FFT. It's a step up in complexity, but it can do this and the order m squared will go to order m log m. The advantages of non-uniform FFT is that it can handle processes that are far more complicated than geometric Brownian motion. So, anything that has a characteristic function you can handle. But, if you, as we do here, we have a Gaussian density, you can do better than a non-uniform FFT. You can use something called a fast Gauss transform which is a method that's designed specifically when you're looking at convolution over Gaussians. The fast Gauss transform was developed in one of the by Greengard and a bunch of other guys and a nice paper written by Mark Broadie, and I think Yamamoto introduced this a long time ago to finance. For some reason, it hasn't really caught on. I don't understand why. It's a brilliant method. And it basically the main idea is if you have something that looks where you have these exponentials, so it's basically a Gaussian convolution that you can reduce the effort from Here, I have uh I have m j uh target points and I have m k source points. Instead of doing this in order m k * m j, you can do it in order m k + m j, which is remarkable. There's not even a log term there. It's a it's so much faster to do this in this space. And this is perfect because the integrals that I've written up and the discretization are exactly in this form. So, relative to some of the other algorithms you will see out there, by writing it like this, you get this order n * n rather than order n * m squared. And, since this is m is normally about 100 or thereabouts, we're looking at two orders of magnitude gain, so by just switching into this clever way of doing it. Alright. So, that's the the basic idea is is you discretize these sequential integrals. And then, you use the fast Gauss transform to execute it like thunder and lightning. You still have to define a quadrature rule. And it turns out you need a scheme that can handle both an infinite domain and sort of half infinite domain for reasons that I'll mention in a second. We're going to use the double exponential quadrature rules that Mark and I have used for a lot of applications, and that's a there's a lot of good reasons for that. They are very straightforward to set up, they're much simpler than say Gaussian quadrature. They're exponentially convergent. They are optimal in a certain cool way, and they are also very robust in terms of dealing with singularities, and whether the interval is half infinite or or infinite or open, closed, and so forth. There are schemes for all of them. And, they have turned out, at least in our experience, to be fantastic for for all finance applications. We'll see the recommendations we come up with. There are a couple of things that need to be considered here. One is that you want ultimately in a double exponential integration scheme, you put a a transformation on that makes everything decay double exponentially. So, exponential decay where the decay itself is exponential. But, in the integral we already have a Gaussian, so the Gaussian decays single exponentially. So, you have to be a little careful in some when you pick the schemes to make sure you don't end up with a triple uh triple exponential scheme, which is definitely not optimal. It might save So, double exponential sounds good. Shouldn't triple exponential and quadruple exponential be even better, but it isn't. The optimality only works for double exponential, so you have to be careful not destroying it. You also need to take into account that because of the dividends uh they will introduce kinks in some of these uh dividend specifications that I that I showed you. They have kinks or or discontinuities to avoid going through zero. You have to identify those and make sure you break the intervals up into segments. Otherwise, if you don't you're going to lose this super rapid spectral convergence. So, anyway, that is uh So, the numerical results here I'm just going to talk about it. If you use a contemporary implementation of fast Gauss transforms, not the stuff that was done back in uh Broadie Yamamoto days, but if you use plain wave expansions rather than Perlmutter expansions this stuff we can get something like 200,000 options per second per dividend interval. So, if you have a one year option with three dividend days, which is about the average in the US, you we can do about 50,000 options per second at a precision of 10 to the -8. If you don't want 10 to the -8, you can do faster than that. So, and this convergence is fractal. So, if we double the effort, we get twice as many significant effort in precision uh digits in precision. So, there's lots of extensions that be done can be done here. We can American puts with dividends. Noisy dividends, so you can introduce dividend models that have noise. If you believe Some of these models say we know for sure what the dividend is going to be if you just tell us the stock price. That's not realistic. But, we can add noise to the dividends. We can do lag dividends. So, if the dividend is determined before it's it's announced, before it is paid, you can do non-zero borrow costs. So, if the drift stock is not r, but you have dividends as well as a dividend yield which is a borrow cost, and you can handle jumps and simple local volatility and so forth in this framework, primarily through non-uniform FFT methods. Those very fast wide range of realistic dividend formulation can be supported. You don't have to penalize yourself down to something that's very simple because you want to rely on some hokey formula for the Europeans. But, we cannot handle local volatility in this model. And this one can become computationally challenging for American puts. It's really good at the calls, and it's very good for the puts, but can sometimes be a little slow. Still very competitive though, and it's super precise. Last topic to just cover what we have not been able to do, what is eluding us at the moment. And this is called new tricks for finite differences. You can see a bunch of references here about this. So, here the basic message is if possible, you should use an integral method. So, that's one of the two methods I mentioned before. Either an integral around the boundary or a integration-based convolution method. These are accurate, stable, fast converging, smooth, easy to deal with in general. No surprises. No uh no issues with rounding errors. Nothing can go wrong if you set them up well. But, they have limitations. In particular, they do require some uh some some orderliness in in the process. Namely, you should have either the characteristic function known or the density known outright. And, you should ideally also know what European option prices are. So, as I mentioned, Black-Scholes like dynamics are sort of that is still the bread and butter for simple American option pricing, especially in markets. But, you could also price Americans using these local volatility models. But then and this is normally a non-parametric local volatility model. So, the the the vol here is a function of both time and stock price but not a particularly smooth or well-behaved. You have something like that, it's very hard to write down anything analytical. And then, you have to look at something else, and this is when you you can either do Monte Carlo simulation or you do a lattice method, where you discretize the process and stop it on a lattice. The two most common lattices lattice methods are the binomial tree or the finite differences. Both have been deployed since the 1970s. In practice, in theory, the binomial tree should have been you know, retired a long time ago, should be resting on the in the graveyard, but it doesn't. It's still very much in in use. And sometimes, or better, preferred to finite differences, even though it has typically has inflexible node placement rules. You have to do a lot of surgery to make it sort of to put the nodes wherever you want. It's generally poor convergence. This is a first order method and if you're not careful, you can get odd even effects and and so forth. Uh, in particular, this fourth first order convergence is not great because there are other defined finite difference methods that are second order convergent, and with that I mean if you take more and more time steps, you will get second order convergence in a finite difference method, but not in a binomial tree. But, yet, the binomial methods are there in practice. One reason is they're easy to implement, the other reason is they are fast, and even if they're not really accurate, they don't converge well, maybe they are sort of fast enough in the sense that maybe they get that 1 cent accuracy or something, it's good enough. or something like that. And, there's also a lot of tricks that you can do to make the binomial trees better that people that started out maybe with a simple binomial tree they have sort of been improving them. But, the reason I want to look at here, and that is that the main composition to binomial tree, namely finite difference grid methods are not doing all that great for American options if you take a look at them. They are harder to implement, they have a higher computational cost for a given number of time steps, the convergence is not great for American options in practice, and it tends to produce oscillating Greeks. So, it doesn't look like it's a great contender here. But, in the next few slides, I'll show you how you can fix all these things. And then, you can come up with a finite difference method that is vastly converging, smooth, etc. etc.. And that's what we're going to do here. So, I'm going to look at the Black-Scholes model again. This is not one you want to use this model again. If you have a model like this, use one of the other two approaches. But, uh I'm going to use it. You really want to then ultimately replace this with something else. I'm going to look at it because that's where all the benchmarks are in the literature and so forth, so I'm going to look at it. I'm going to look at the put option again and I'm going to again have some kind of boundary in this space that I know separates uh exercise from non-exercise. So, this is the same set up as we had before. I'm moving into log space and that gives me this sort of I have a a uh I have a PDE that I solve when I am in the continuation region, and then below that the value function of the put is just equal to the payout function. So, little x's here just means that my that is everything is restated in x coordinates, or log coordinates here. And here's the uh the partial differential equation uh operator. So, that's uh basically the same as I said before. The idea of finite difference grids is to take a PDE such as this one and replace everywhere you see sort of differentials uh both in time and in space, replace them by finite difference approximations. Something like When since this space this operator l of x is approximately equal to a discrete operator, where you just move instead of whenever it says df dx you just sort of do f of x + delta - f of x divided by delta x, something like this. And then, you you try to stand up things. So, basically finite difference coefficients um rather than real differentials. And then, you write that down, and then you say, I'm going to use that instead. So, that leads to a scheme where this operator here will be written as a sum of three weights, or alpha, beta, and gamma being the weights of um of the of the function f but evaluated at x points that are shifted one delta x up and one delta x down. So, I now have get something that relates three points together, so much simpler than a differential. This originates out of the out of the uh you you write down these difference equations, and then you get a simple system like this. And then, you can do the same for the uh time derivative, and then when you write the PDE down this one. So, you take the the time derivative will have its finite difference approximation and so will the all the spatial derivatives, and then you get an equation like 28. You'll see I've introduced a parameter theta, and there's a very classical way of uh discretizing that simply chooses where uh in time. So, if you're operating on an interval from tau prime to tau prime + delta t, then you the spatial operator can be done either at t prime or at t prime + delta t, and in this theta scheme, you weight you weight them by this parameter theta. And, depending on which one you choose, theta = 1 is the fully implicit scheme, theta = 1/2 is the Crank-Nicolson, and theta = 0 is fully explicit. The fully implicit scheme is nice and smooth, but doesn't converge very well in the time domain. Crank-Nicolson is the one we really want to use because it's second order of convergence in time. So, if you add more time nodes, it will converge faster. The fully explicit, it's not something I would recommend because it's super unstable in general, unless you have a lot of time steps. And, this When you arrange these equations on a grid, then you basically get a a system of matrix equations, where you have a a tridiagonal system with these alpha, beta, gamma, and then you can sort of step through a grid solving matrix equations each each step. So, you start at the known boundary, and then you step your way forward in tau space, which is actually backward in calendar time space, solving nice uh tridiagonal system. So, it leads to an easy to compute uh system because uh inverting these tried diagonal matrices can be done at linear complexity, using  LU decomposition. And, if you have n time steps, then the total work is order n * m. So, that is similar to the uh um to the work effort of the convolution methods. The only difference is here that the convolution method is kind of exact, whereas this is discretized. Um, so, they look the same but they're not not the same. One is a discretization that is much coarser. So, um another thing to notice is in the way you set up equation 28, is that you end up when you're looking at it at a particular sort of node here tau i, x i in the grid, you will see that you actually that that function will end up depending on sort of the six neighboring points in this particular manner here. And then, when you blow that up to the entire grid, that's how you end up with a matrix equation. This flow here, we'll see it in in a second. So, here is uh uh LU decomposition. High level, what that involves is you do sweep upwards, and then you compute some constants, and then you sweep downwards uh to complete the calculations. So, that's how it works. So, you basically compute sweep up in the grid, and then you have a bunch of constants, and then you iterate your way back, and that solves the matrix equations. ULD composition is less standard, but it works the other way. The first sweep is downward, and the second sweep is upward. So, now what I've shown you is the way to solve the finite difference grid. You need some way to capture the early exercise, and the simplest solution here is basically use that Bermuda approximation, where you say, okay, I'm going to try to just assume this is Bermuda. So, what I do is in my grid, I roll one step, and then I take And I assume that that's kind of the then I maxed that over the uh over the payout. So, that's pretty much what you're doing uh You kind of take one step back and say, that's my holding value, and then you max the whole thing against the payout. So, that's written out here as how you would do that. That is terrible because for reasons I I told you, a Bermuda approximation will approximate only linear in in the time steps. So, no matter what you do, this will destroy the convergence. So, even if you think you are using something good with Crank-Nicolson, doing this will ruin the the convergence steps. So, we can do better than that. There's a lot of methods that you can do here, um that I've listed here. Policy iteration, P SOR and so forth. They uh they are all generally uh uh pretty involved, but there's a simple method to do it, and that is called the Brennan and Schwartz algorithm. It does The basically, you do a standard ULD downwards step. So, you do the sweep down, and then you start as you roll back up, you start deploying the max inside the loop. So, you don't go up and down and then max. You go You go down, and then you start rolling in the max. This turns out to make all the difference. And as it turns out, trick number one here is that this algorithm can be sped up quite a lot. I'll show you here how it works in So, this is the explicit method. You go up, down and max. The classic Brennan and Schwartz, you go down, and then you go up, but you start doing the max. The new trick that I'm advocating is that you as you sweep down, you actually there's a way to find out where this max operation ceases to work. So, you basically there's that's the point where these maxes will identify Okay, now I am out of the of the uh of the exercise region. You don't have to go all the way down to find that. So, you can stop. And then, you can roll up here and all the values in this below here you don't have to compute them because they're in the exercise region. You can just fill in the exercise value. So, this on average saves you about half of the time by doing this. I'm not sure why people haven't not advocated this, but right there, you uh you cut significantly factor up to two out of the whole thing by just doing it this way. Here's a little test case where I try to I run a test case, and then I see a variety of test cases, where I start with an implicit scheme, and then I use like a bad exercise strategy and so forth. The mostly most interesting thing to notice on this is most of these graphs are slope 1. The one that I had hoped would be slope 2 down, so second order of convergence, this is the one where I use a Crank-Nicolson and I used the fancy fast Brennan and Schwartz, it isn't it's slope 1.4 downward. So, that's a bummer. It's faster, obviously, and better convergence than the other methods, but it isn't a slope of 2. And, also, if I look at the estimated exercise boundary that comes out of the grid, so I have the exact boundary in red, and I'll see that the for uh sort of average size finite difference grid, I get a very choppy estimate of the exercise. Just because of discretization effects and it's particularly bad in the short term. And if I look at the gamma that comes out of this method, that I thought was great, it's really bad if you look at it. That's the gamma. It's like a stormy seas. And, here's another slice. In So, it was very typical of Crank-Nicolson methods. So, here the problems are listed here. I don't get second order convergence, I don't resolve the exercise boundary, the gamma is terrible, and overall doesn't look so great relative to a binomial tree. But, they can all be fixed. The first fix is very simple: move into a power transformation. So, instead of using an equidistant grid, move into a grid that is spaced according to some kind of power law, tau to a power less than 1. And something like 1/2 is good. Uh, if you do that, and the reason for that is that the exercise boundary is known, as we had this big swing as you get close to maturity, or close to zero, in tau space. So, that is because it behaves almost like a like a square root. So, you can mimic that by using a power transformation to do a different spacing. So, you put more spaces in the beginning of the grid around where there's a lot of movement, and that's uh as it turns out, that fixes that. So, here's If I just do that, and suddenly you'll see I can get slopes that are actually all the way down to -3, which is better than what I hoped for which was minus minus 2. So, that's just doing that. And, the other is to and try to estimate the exercise boundary to subgrid precision as you are instead of having these sort of this discretized things. There are ways you can do inside the finite difference grid as you run the methods to discover to find where it actually lies. And, very often, it is in between points. Very easy to do. No extra work, but it does mean you can locate it. You'll see in the paper how to do it, but it's basically just an interpolation. And, here is the green one is the old method, before I tried this, and the black one is after I do this in interpolation. Now, you can see even in the short end where you have a lot of movement, I now track it because I've done interpolation and I've done square root spacing. So, now I'm resolving that really well. And then there's the gamma noise, which is the most problematic part. I'm not going to spend a lot of time on it but the basic idea is here to, it originates because when I'm trying to at a particular point in time to to apply this finite difference grid, I end up with this sort of six point flow and I start having the dependency. When I'm above the exercise value, I'll start dipping below it one time step before, and that means that when it comes to the gamma, you're going to see a lot of bad effects if you're crossing over the gamma is discontinuous at the boundary, as it turns out. And when I do this stuff, I'm I'm in trouble, because I start applying the PD at a point where it shouldn't be used and I'm dipping over a singularity or a discontinuity. I can avoid that, and this little involved how to do this, but you you identify a point, if you know where you are here and there you identified a point, that says, this is right where I'm going to at this level of X given that where I'm going to cross the exercise boundary. And then, you rewrite the discretization and you use a particular rule or particular known property of points that are right at the exercise boundary, then you actually get a modification of the difference scheme for points, all the points that sort of dip into something you're going to modify the final difference grid locally, and then you avoid spilling into it. And that will fix it. And here's the gamma with that, and it takes no time as well. All of these are little fiddle things that you're doing at the final difference grid and none of them add up to adding more time. And it fixes the problems. Another way you can actually save time is to do one final thing here, and that is the final difference grid normally sits in a rectangle. But there are parts of it where you are wasting time. For instance, if you have the exercise boundary, we outlined first, a way to avoid ever going below the exercise boundary. This scheme will stop once it sees it, so there's a good chunk that you're not going to go underneath at all, because you can stop the final difference grid. Then there are parts of it that are statistically irrelevant. What's like a final like a Bermuda trees like a triangle, there are pieces of the final difference grid you can lop off. And then there are pieces where there's no gamma, which is this piece up here. For the option, this for all practical purposes, a straight line. If you lop up all this, you're left with this little sliver here in the middle, and that's where you solve it. So now, this alone can chuck off a lot more time, as well, make it faster. And this numerical performance, I'm comparing it against F5, which is a final difference grid method by Peter Forsyth and co co co-authors that is sort of the gold standard. And that paper is demonstrated it demonstrates that it can beat binomial trees. Years with all these tricks I we end up beating this method by about a factor of 25. So, I'm not talking about beating twice as fast, or three times as fast, I'm talking about being 25 times faster. It is also simpler, because this method in F5 has a penalty iteration going in adaptive grid strategy and so forth. None of that. It has stable gammas, which this paper does not have. And I can set my time and assets steps anywhere I want. I don't have this restriction on trying to keep everything very uh um in the Crank-Nicolson grid, you have to be very careful, in general uh if the time step is is too high, because of these oscillations that can come around boundaries. But because I've avoided getting into boundaries, I can set my time steps in assets anywhere I want. And then, as a bonus I get this really nice high precision estimate of the exercise boundary that I can use for other things. One has two or three more tricks, if you want to recrank it up, but I'm basically done, and in summary here is just my recommendations. Boundary integration, you use it when the stock process is simple and there are no dividends except when you have simple proportional American calls with general jump dividends, you use fast cross transform. The process is not geometric Brownian motion, you can use the non-uniform FFTS, and if everything else fails you use the tricked up Crank-Nicolson final difference grid. And I think that kind of covers the situations that I think you will see in practice. Um, I guess uh I don't think anything is going to not fall into this be picked up by this sieve of things. But hopefully between all these things, that should be independent of your application, there's something you can pick up, and I think we uh we hold it there, and now I'm 5 minutes late. Okay. Okay. Alright. Thank you. We can start the Q&A session now. Okay. Okay, we will start the Q&A from the room, and then go to WebEx. Okay. If anyone on WebEx has a question, please uh click the raise hand button. Um, I'm just going to read the compliance statement again. Uh this talk will be open to a number of book for clients who will join us online. Given that individuals present from different organizations please do not share or discuss any sensitive or confidential information or materials that could result in a conflict of interest or any uh competition legal concerns. Steve. Yep. Yep. Uh I I have some comments uh not really question, uh thank you lot uh Lie for giving us such insight for a very rich content uh talk about very uh I'd say that's traditional old topic, but I think the timing is perfect, uh uh uh there is a big renaissance of uh research in this area, because the business needs, actually just before your talk, I just had a meeting with uh and our team are meeting with E*TRADE. Yep. Talk about the needs for improve our American options for the needs, including handle dividends or and uh handling tax rate and so and so forth. Yep. Which confirm your uh assessment that we need a general uh framework, not only one model. Maybe specific model for different purposes. Yep. Yeah. So, I think uh uh uh that's a general uh uh comments about uh the timing uh uh uh of this uh kind of research. Uh my question specifically about uh the 25 uh times speed up. That's also include LocalVol. LocalVol uh uh Yeah, these are these are all run for They're running geometric Brownian motion to give a to give a benchmark, but I did not in none of the tricks that I put in here depend whatsoever on the process being a geometric Brownian motion. There are a couple of uh the the ones that I mentioned, if you want to see some more tricks, some of them are specific to geometric Brownian motion, but I didn't list them here because they are universal. They that I rely nowhere in any of this, do I rely in the process being uh a uh geometric Brownian motion. As uh as uh when I'm saying here is also in this particular so the question is, I I find it interesting thing about will we use LocalVol for American options? Right. Right. <noise> To my understanding the way the market quotes are, they may not actually, I don't know that that is a market standard. But I also know that if you are interested in for instance, fitting a LocalVol function and you know that the quotes are American, right now we de-Americanize, I think the entire streets will just take the uh the Ameri the the quotes and then they will de-Americanize the American quotes, turn them into Europeans, and then they fit their LocalVol functions. If you want to actually fit your LocalVol functions directly on an American option, you need to have something that can price Americans using LocalVol. Right. And then you need to think about how you're going to embed that and so the last approach here will is designed specifically with that application in mind. Now, is that going to be uh it I I I I'm not sure that you will ever quote right off LocalVol to Americans but you will I'm sure you'll use it in sort of the fitting of LocalVol models for uses elsewhere, and will be part of the hedging calculation, and so forth. Yeah. Right. Thank you. Okay. Any other questions? Yes. Question, yes. Yeah, for the for the for the your latest uh new methods, how does it work under the case of negative interest rates? I I remember you had a before a paper, right? In this case the exercise boundary is more complicated, isn't it? Yes, so so what happens, so in the first approach here, boundary integrate which requires you to explicitly calculate the boundaries. You need to then take into account that the boundary under some conditions will bifurcate into two. We cover that for for that method. For these methods here, we don't actually need the bond, because they proceed more or less like a like a binomial tree, that you just loop them back and then you run over and see uh you know, are they bigger than the exercise values or not, so it's basically like a Bermuda-n thing. And then you can ask subsequently, where was that exercise boundary, by the way, and it may be that there were two points. But you don't explicitly uh use its location. And the same thing goes for a finite difference grid per se. There you you will have to this the branching spot algorithm will locate it, and then, you will have to modify that algorithm if there's a bifurcation, you know there's going to be one above and below. So then you will run that LUU decomposition has to be done a little bit different, so you do both cases. But that's actually covered in the literature, how to do that, to make sure that now you have both up and down. There may be one above you and one below you. Sometimes, there are two of them, two points. So you can't necessarily stop. But I would say the case of negative interest rates was more interesting to me a year or two ago when rates were were now is like I don't know. That was probably one in humanity, we saw that. So, I'm not sure I'm going to We have decided in some of these cases to remove the ability just simulate, but we can add it back uh just in case we ever needed it, with some complication. Some cases, no complication, other cases we'll have to be aware of it. But, um so at this point I think it's probably the least of our concerns on negative interest rates. But, yeah. Any other questions? Yes? So, uh in in your PD method, you you're using uh square root uh of time steps, right? Yeah. We use square root time, as a as a matter of fact, maybe toward to this point, power of 0.6 or something like that. This is This is without the dividends. If you have dividends and uh you need to re-credit. So if you have if so this particular so if you're doing the square root, you would only if you have dividends, in that case, right, the exercise boundary obviously will change in the in for that particular case. You will need something for the last slice, but for the other slices, in many cases, it doesn't look anything like like that. That's that's that's correct. Yeah, but you will still the the algorithm will still find it and you will probably still use square root spacing just because at the very last, as you get to a maturity, you will always have that kind of square root behavior. Can you Can you use the uh boundary integration method on uh if it you know, from maturity to the  dividend? Yes, so hum it's a good question, so the the question is whether you can can you combine some of these methods by using the first piece or if you know that you have, if an exercise is in between, for instance, for the put you'll see that there are discontinuities. But an exercise boundary does sort of emerge in between. Can you then uh can you combine methods and use maybe a a piece of of an integration, boundary integration and then a piece of convolution or a piece of finite difference grid. And the answer is, yes, you can, but you know, it starts getting it starts getting complicated. And if you are in the situation where things are unclear to you, in that got I would go to a finite difference grid because some of the things are more mechanistic. There, if there's an exercise boundary it will find it. fast. If it isn't there, it just it won't find it. It it won't be there. So, and sometimes, these exercise boundaries can pop in and out. But if you have discrete dividends and puts, sometimes there is no nothing in between. And then suddenly, it will cop up again. They they can be very complicated and there's a possibility in between uh dividend dates. So, depending on the style of dividends that you have. Uh I have one more question, but fast Gaussian transform, so it's could be extended to to um the call with borrow cost where you you have some Yeah. sub-optimal uh exercise. Yeah, so so the extension to borrow cost is I didn't put it in here because there are complications around it. It depends on the borrow cost. If the borrow cost is negative then the call options, for instance, retain the Bermuda-n feature. If the borrow cost is positive, so that as if there is a continuous yield, then in between these days, a a an exercise boundary may emerge in the in for this case. So if you want to use a convolution method, the only way to capture those kinds of events, which for call option tends to be, borrow cost has very, very limited effects on uh on a call option unless it's really big. If you already have a discrete dividend, but then, what you can do in that space is that you would use these methods but you would have to chop it down and use some kind of stitching and extrapolation on the method, because you know the convergence is uh from the Bermuda-n to the American follows a particular pattern. That's the only way you can you can do uh you can use these convolution outright. You have to treat things as a Bermuda-n, and then, you extrapolate to the limit, or you move into a method that you know where you don't need to do that. But the borrow cost is is a complication because it can make a an exercise boundary emerge in between dates for a call. So, you would probably have a lot of experience with that. I don't think it's a big effect, because the borrow cost tends to be relatively small, so.  And now we have put up options where we have cash dividend. So, put up options with cash dividends, so there are three approaches, right? Depending if depending on what kind of dividend it is. If it is a proportional dividends, then you can use the uh the the integr the first integration approach. If it is not a proportional integral, what you can do is you can use the convolution method but you have to use some kind of extrapolation to the limit because it is not a Bermuda-n, it is not it is somewhere in between. Or, you can use the last method, will handle that or anything else without any problem. Finite difference grid uh any kind of dividend can easily be modified to that situation, so. So you would have to make a choice uh and that's something that you have to run a benchmark test. And it also depends on, and if it's Local Volatility if the choice is made for you. If it's geometric Brownian motion, then uh you know, you may be able to the extrapolation approach might might work perfectly fine. If if there are no more questions from the room, uh I think we'll pass it to WebEx. Okay. So, for people on WebEx, uh I mean there is a raise hand button on the right uh on your WebEx. Yeah, if you do have any questions, please uh click the button. I will unmute you. So there is a question from Julie. Yeah, please go ahead. Um about the economics. Uh, looking in FX uh trading history for LCT, I see that European options are thousand times more popular. And it seems that in equities it's the American options that are more popular. Where does the difference come from? Yeah. Why is that? You just uh in equities for all all options on index is more European, but uh uh corporate, each individual corporate, it is all American options. It's just uh that kind of uh originally the convention. And uh have you <noise> <noise> Yeah, I think uh uh I think the idea uh is for corporate options, most of them are American. If you talk about the index, it's more European and also for the liquidity concern. Yeah, liquidity concern is a factor in that, yeah. They're not being traded as actively as the European options. Yeah. They are uh <noise> Yeah. Yeah. And the American uh they they are sort of the uh more uh <noise> a specific purpose. So something like that. We know not every market has no no There's no <noise> <noise> <noise> <noise> <noise> <noise> <noise> <noise> <noise> <noise> <noise> Okay. Any more questions from WebEx? No, no. Okay. There's no questions from the room either. I will watch you on now. Okay. Um so yeah. I think uh uh the the issue is there, and the uh the the market is very, very, very, very quickly <noise> to catch up on this space pretty swiftly. You can go through a few of the things to make sure that the price is correct. I think we are very, very quickly getting close to a situation where we have to move away from that, and I'm sure the market will adapt to that and eventually just make everything be Americans. \n"


